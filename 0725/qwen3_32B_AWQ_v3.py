"""
ê³ ì„±ëŠ¥ ë³‘ë ¬ íšŒì˜ ìš”ì•½ ì‹œìŠ¤í…œ
- í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸: íŒŒì¼ì½ê¸°(ìŠ¤ë ˆë“œ) + GPUì²˜ë¦¬(í”„ë¡œì„¸ìŠ¤)
- ë©”ëª¨ë¦¬ ìµœì í™”: ìŠ¤íŠ¸ë¦¬ë° + ì œí•œëœ í + ëª¨ë¸ ê³µìœ 
- ì„±ëŠ¥: GPU ë°°ì¹˜ì²˜ë¦¬ + ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸
"""

import os
import json
import re
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import threading
from queue import Queue, Empty
import time
from functools import partial
import logging
from typing import List, Dict, Tuple, Optional
import argparse
from glob import glob
from tqdm import tqdm
import torch
from transformers import AutoTokenizer
from awq import AutoAWQForCausalLM

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('summary_process.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class GPUPerformanceMonitor:
    """GPU ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.start_time = None
        self.processed_chunks = 0
        self.processed_files = 0
        self.total_tokens = 0
        
    def start(self):
        self.start_time = time.time()
        logger.info("GPU ì²˜ë¦¬ ì‹œì‘")
        
    def update(self, chunks_count: int, tokens_count: int, files_count: int = 0):
        self.processed_chunks += chunks_count
        self.total_tokens += tokens_count
        self.processed_files += files_count
        
        if self.start_time:
            elapsed = time.time() - self.start_time
            chunks_per_sec = self.processed_chunks / elapsed
            tokens_per_sec = self.total_tokens / elapsed
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_cached = torch.cuda.memory_reserved() / 1024**3
                
                logger.info(f"ì§„í–‰ë¥ : {self.processed_chunks}ì²­í¬, "
                           f"{self.total_tokens}í† í° ì²˜ë¦¬ "
                           f"({chunks_per_sec:.1f}ì²­í¬/ì´ˆ, {tokens_per_sec:.0f}í† í°/ì´ˆ) "
                           f"GPUë©”ëª¨ë¦¬: {gpu_memory:.1f}GB/{gpu_memory_cached:.1f}GB")
            else:
                logger.info(f"ì§„í–‰ë¥ : {self.processed_chunks}ì²­í¬, "
                           f"{self.total_tokens}í† í° ì²˜ë¦¬ "
                           f"({chunks_per_sec:.1f}ì²­í¬/ì´ˆ)")
    
    def finish(self):
        if self.start_time:
            total_time = time.time() - self.start_time
            logger.info(f"ì™„ë£Œ: {total_time:.2f}ì´ˆ, "
                       f"ì´ {self.processed_chunks}ì²­í¬, "
                       f"{self.total_tokens}í† í°, "
                       f"{self.processed_files}íŒŒì¼")
            return total_time
        return 0


class ModelManager:
    """ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”©"""
    
    _instance = None
    _model = None
    _tokenizer = None
    
    def __new__(cls, model_path: str):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, model_path: str):
        if self._model is None:
            self.model_path = model_path
            self._load_model()
    
    def _load_model(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (í•œ ë²ˆë§Œ)"""
        logger.info(f"ëª¨ë¸ ë¡œë”© ì‹œì‘: {self.model_path}")
        
        try:
            self._tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            self._model = AutoAWQForCausalLM.from_quantized(
                self.model_path,
                fuse_layers=True,
                trust_remote_code=True,
                safetensors=True,
                device_map="auto",
                offload_folder="./offload"
            )
            
            logger.info("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    @property
    def model(self):
        return self._model
    
    @property
    def tokenizer(self):
        return self._tokenizer


class ParallelSummaryProcessor:
    """ê³ ì„±ëŠ¥ ë³‘ë ¬ ìš”ì•½ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, model_path: str = "Qwen/Qwen3-32B-AWQ", 
                 max_workers: Optional[int] = None):
        self.model_path = model_path
        self.max_workers = max_workers or min(4, mp.cpu_count())  # GPU ë³‘ëª© ê³ ë ¤
        self.monitor = GPUPerformanceMonitor()
        
        # ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.model_manager = ModelManager(model_path)
        
        logger.info(f"ë³‘ë ¬ ìš”ì•½ ì‹œìŠ¤í…œ ì´ˆê¸°í™” - GPUì›Œì»¤: {self.max_workers}ê°œ")
    
    def _load_jsonl_utterances(self, file_path: str) -> List[Dict]:
        """JSONL íŒŒì¼ì—ì„œ ë°œì–¸ ë°ì´í„° ë¡œë“œ"""
        utterances = []
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        data = json.loads(line.strip())
                        if "text" in data and data["text"].strip():
                            utterances.append({
                                "speaker": data.get("speaker", f"speaker_{line_num}"),
                                "text": data["text"].strip()
                            })
                    except json.JSONDecodeError:
                        logger.warning(f"Line {line_num}: JSON íŒŒì‹± ì˜¤ë¥˜")
                        continue
        except Exception as e:
            logger.error(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        return utterances
    
    def _chunk_text_with_speakers(self, utterances: List[Dict], 
                                 max_tokens: int = 5000, stride: int = 3500) -> List[Tuple[str, List[str]]]:
        """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì²­í‚¹ with ë°œì–¸ì ì¶”ì """
        tokenizer = self.model_manager.tokenizer
        
        input_ids = []
        metadata = []  # ê° í† í°ì˜ ë°œì–¸ì ì¶”ì 
        
        for utt in utterances:
            try:
                tokens = tokenizer.encode(utt["text"], add_special_tokens=False)
                input_ids.extend(tokens)
                metadata.extend([utt["speaker"]] * len(tokens))
            except Exception as e:
                logger.warning(f"í† í°í™” ì˜¤ë¥˜: {e}")
                continue
        
        if not input_ids:
            return []
        
        chunks = []
        i = 0
        while i < len(input_ids):
            chunk_ids = input_ids[i:i + max_tokens]
            chunk_speakers = metadata[i:i + max_tokens] if i < len(metadata) else []
            
            try:
                chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)
                unique_speakers = list(set(chunk_speakers)) if chunk_speakers else ["unknown"]
                
                chunks.append((chunk_text, unique_speakers))
                
                # ë§ˆì§€ë§‰ ì²­í¬ë©´ ì¢…ë£Œ
                if i + max_tokens >= len(input_ids):
                    break
                i += max_tokens - stride
                
            except Exception as e:
                logger.warning(f"ì²­í¬ ë””ì½”ë”© ì˜¤ë¥˜: {e}")
                i += max_tokens - stride
                continue
        
        return chunks
    
    def _generate_summary(self, prompt: str, max_new_tokens: int = 1024) -> str:
        """ìš”ì•½ ìƒì„± í•¨ìˆ˜"""
        model = self.model_manager.model
        tokenizer = self.model_manager.tokenizer
        
        try:
            enc = tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                return_attention_mask=True,
                max_length=8192  # ìµœëŒ€ ì…ë ¥ ê¸¸ì´ ì œí•œ
            ).to("cuda")
            
            with torch.no_grad():
                output = model.generate(
                    input_ids=enc.input_ids,
                    attention_mask=enc.attention_mask,
                    max_new_tokens=max_new_tokens,
                    pad_token_id=tokenizer.eos_token_id,
                    temperature=0.1,
                    do_sample=True,
                    top_p=0.9
                )
            
            result = tokenizer.decode(output[0], skip_special_tokens=True)
            
            # í”„ë¡¬í”„íŠ¸ ì œê±°
            if prompt.strip() in result:
                result = result.replace(prompt.strip(), "", 1).strip()
            
            # 'ìš”ì•½' ì´í›„ ë‚´ìš©ë§Œ ì¶”ì¶œ
            match = re.search(r"(### ìš”ì•½[\s\S]*)", result)
            return match.group(1).strip() if match else result
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}"
    
    def _build_summary_prompt(self, chunk: str, speakers: List[str], 
                             summary_accum: str = "") -> str:
        """ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        participants_str = ", ".join(speakers)
        
        return f"""ë‹¤ìŒì€ íšŒì˜ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.

[ì°¸ì—¬ì]
{participants_str}

[íšŒì˜ ì²­í¬]
{chunk}

[ì´ì „ê¹Œì§€ì˜ ìš”ì•½]
{summary_accum}

1. ì´ íšŒì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜.
2. ì•ˆê±´ì´ ìˆë‹¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•´ì¤˜.
3. ê° ì•ˆê±´ì— ëŒ€í•´ í•„ìš”í•œ ì‘ì—…ë“¤ì„ ë¶„í•´í•´ì¤˜. (ëˆ„ê°€, ë¬´ì—‡ì„, ì–¸ì œê¹Œì§€)

ê²°ê³¼ëŠ” ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¤˜:

### ìš”ì•½
- ...

### ì•ˆê±´
1. ì œëª©: ...
   - ì„¸ë¶€ ì„¤ëª…
   - ê´€ë ¨ ë°œì–¸ì

### ì—…ë¬´ ë¶„í•´
- [ì—…ë¬´]: ë‹´ë‹¹ì, ë§ˆê°ì¼, ê´€ë ¨ ì•ˆê±´
""".strip()
    
    def _process_file_batch(self, file_batch_data: List[Tuple[str, List[Dict]]], 
                           max_tokens: int = 5000, stride: int = 3500,
                           max_new_tokens: int = 1024) -> List[Dict]:
        """ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ì—ì„œ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬"""
        results = []
        
        for file_path, utterances in file_batch_data:
            if not utterances:
                continue
            
            try:
                # ì²­í‚¹
                chunks = self._chunk_text_with_speakers(
                    utterances, max_tokens=max_tokens, stride=stride
                )
                
                if not chunks:
                    continue
                
                # ìˆœì°¨ì  ìš”ì•½ (ëˆ„ì )
                summary_accum = ""
                file_results = []
                
                for idx, (chunk, speakers) in enumerate(chunks):
                    prompt = self._build_summary_prompt(chunk, speakers, summary_accum)
                    response = self._generate_summary(prompt, max_new_tokens)
                    
                    result = {
                        "file": os.path.basename(file_path),
                        "chunk_index": idx,
                        "response": response,
                        "speakers": speakers,
                        "tokens": len(chunk.split())  # ëŒ€ëµì  í† í° ìˆ˜
                    }
                    
                    file_results.append(result)
                    summary_accum += response + "\n"
                
                results.extend(file_results)
                
            except Exception as e:
                logger.error(f"íŒŒì¼ {file_path} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        return results
    
    def process_hybrid_parallel(self, input_pattern: str, output_file: str,
                               batch_size: int = 2, queue_maxsize: int = 4,
                               max_tokens: int = 5000, stride: int = 3500,
                               max_new_tokens: int = 1024) -> bool:
        """í•˜ì´ë¸Œë¦¬ë“œ ë³‘ë ¬ì²˜ë¦¬: íŒŒì¼ì½ê¸°(ìŠ¤ë ˆë“œ) + GPUì²˜ë¦¬(í”„ë¡œì„¸ìŠ¤)"""
        
        file_paths = glob(input_pattern)
        if not file_paths:
            logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_pattern}")
            return False
        
        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ë³‘ë ¬ì²˜ë¦¬ ì‹œì‘ - {len(file_paths)}íŒŒì¼, "
                   f"ë°°ì¹˜í¬ê¸°: {batch_size}, íí¬ê¸°: {queue_maxsize}")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì œí•œëœ í
        data_queue = Queue(maxsize=queue_maxsize)
        total_files = len(file_paths)
        
        # ìŠ¤íŠ¸ë¦¬ë° íŒŒì¼ ì½ê¸° ìŠ¤ë ˆë“œ
        def streaming_file_loader():
            current_batch = []
            processed_files = 0
            
            try:
                for file_path in file_paths:
                    try:
                        utterances = self._load_jsonl_utterances(file_path)
                        if utterances:
                            current_batch.append((file_path, utterances))
                            processed_files += 1
                            
                        # ë°°ì¹˜ê°€ ì°¼ê±°ë‚˜ ë§ˆì§€ë§‰ íŒŒì¼ì´ë©´ íì— ì¶”ê°€
                        if len(current_batch) >= batch_size or processed_files == total_files:
                            data_queue.put(current_batch, timeout=60)
                            logger.info(f"ë°°ì¹˜ ìƒì„±: {len(current_batch)}íŒŒì¼ "
                                       f"({processed_files}/{total_files})")
                            current_batch = []
                            
                    except Exception as e:
                        logger.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
                        continue
                
                # ì¢…ë£Œ ì‹ í˜¸
                data_queue.put(None)
                logger.info("íŒŒì¼ ë¡œë”© ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"íŒŒì¼ ë¡œë” ì˜¤ë¥˜: {e}")
                data_queue.put(None)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.monitor.start()
        
        # íŒŒì¼ ì½ê¸° ìŠ¤ë ˆë“œ ì‹œì‘
        loader_thread = threading.Thread(target=streaming_file_loader, daemon=True)
        loader_thread.start()
        
        # GPU ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤ í’€
        all_results = []
        process_func = partial(self._process_file_batch,
                              max_tokens=max_tokens, stride=stride,
                              max_new_tokens=max_new_tokens)
        
        try:
            # ê²°ê³¼ íŒŒì¼ ì¤€ë¹„
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            with open(output_file, "w", encoding="utf-8") as f_out:
                with ThreadPoolExecutor(max_workers=2) as executor:  # GPU ë³‘ëª© ê³ ë ¤
                    active_futures = {}
                    completed_batches = 0
                    
                    while True:
                        try:
                            batch_data = data_queue.get(timeout=30)
                            
                            if batch_data is None:  # ì¢…ë£Œ ì‹ í˜¸
                                logger.info("ëª¨ë“  ë°°ì¹˜ ìˆ˜ì‹  ì™„ë£Œ")
                                break
                            
                            # GPU ì²˜ë¦¬ ì‹œì‘
                            future = executor.submit(process_func, batch_data)
                            active_futures[future] = len(batch_data)
                            
                            # ì™„ë£Œëœ ì‘ì—…ë“¤ ìˆ˜ì§‘ ë° ì¦‰ì‹œ ì €ì¥
                            completed_futures = [f for f in active_futures.keys() if f.done()]
                            
                            for future in completed_futures:
                                try:
                                    batch_results = future.result()
                                    
                                    # ì¦‰ì‹œ íŒŒì¼ì— ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
                                    for result in batch_results:
                                        json.dump(result, f_out, ensure_ascii=False)
                                        f_out.write("\n")
                                    f_out.flush()
                                    
                                    completed_batches += 1
                                    file_count = active_futures.pop(future)
                                    
                                    # í†µê³„ ì—…ë°ì´íŠ¸
                                    total_tokens = sum(r.get("tokens", 0) for r in batch_results)
                                    self.monitor.update(len(batch_results), total_tokens, file_count)
                                    
                                except Exception as e:
                                    logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                                    active_futures.pop(future, None)
                        
                        except Empty:
                            if not loader_thread.is_alive():
                                break
                        except Exception as e:
                            logger.error(f"í ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                            break
                    
                    # ë‚¨ì€ ì‘ì—…ë“¤ ì™„ë£Œ ëŒ€ê¸°
                    if active_futures:
                        logger.info(f"ë‚¨ì€ {len(active_futures)}ê°œ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°...")
                        for future in as_completed(active_futures.keys()):
                            try:
                                batch_results = future.result()
                                
                                for result in batch_results:
                                    json.dump(result, f_out, ensure_ascii=False)
                                    f_out.write("\n")
                                
                                completed_batches += 1
                                file_count = active_futures[future]
                                total_tokens = sum(r.get("tokens", 0) for r in batch_results)
                                self.monitor.update(len(batch_results), total_tokens, file_count)
                                
                            except Exception as e:
                                logger.error(f"ìµœì¢… ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        except KeyboardInterrupt:
            logger.warning("ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­")
            return False
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return False
        finally:
            if loader_thread.is_alive():
                loader_thread.join(timeout=10)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ
        total_time = self.monitor.finish()
        
        logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {output_file}")
        return True
    
    def process_simple_parallel(self, input_pattern: str, output_file: str,
                               max_tokens: int = 5000, stride: int = 3500,
                               max_new_tokens: int = 1024) -> bool:
        """ê°„ë‹¨í•œ ë³‘ë ¬ì²˜ë¦¬ ëª¨ë“œ (ì‘ì€ ë°ì´í„°ì…‹ìš©)"""
        
        file_paths = glob(input_pattern)
        if not file_paths:
            logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_pattern}")
            return False
        
        logger.info(f"ê°„ë‹¨í•œ ë³‘ë ¬ì²˜ë¦¬ ì‹œì‘ - {len(file_paths)}íŒŒì¼")
        
        # ëª¨ë“  íŒŒì¼ ë°ì´í„° ë¡œë“œ
        all_file_data = []
        for file_path in file_paths:
            utterances = self._load_jsonl_utterances(file_path)
            if utterances:
                all_file_data.append((file_path, utterances))
        
        if not all_file_data:
            logger.error("ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        self.monitor.start()
        
        # ìˆœì°¨ ì²˜ë¦¬ (GPUëŠ” í•˜ë‚˜ë¿ì´ë¯€ë¡œ)
        all_results = []
        for file_path, utterances in tqdm(all_file_data, desc="íŒŒì¼ ì²˜ë¦¬"):
            try:
                batch_results = self._process_file_batch(
                    [(file_path, utterances)],
                    max_tokens=max_tokens,
                    stride=stride,
                    max_new_tokens=max_new_tokens
                )
                all_results.extend(batch_results)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                total_tokens = sum(r.get("tokens", 0) for r in batch_results)
                self.monitor.update(len(batch_results), total_tokens, 1)
                
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
                continue
        
        # ê²°ê³¼ ì €ì¥
        try:
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, "w", encoding="utf-8") as f_out:
                for result in all_results:
                    json.dump(result, f_out, ensure_ascii=False)
                    f_out.write("\n")
            
            self.monitor.finish()
            logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False


def get_optimal_settings(input_pattern: str) -> Dict:
    """íŒŒì¼ ê°œìˆ˜ì™€ í¬ê¸°ì— ë”°ë¥¸ ìµœì  ì„¤ì •"""
    try:
        file_paths = glob(input_pattern)
        total_size_mb = sum(os.path.getsize(f) for f in file_paths) / 1024 / 1024
        file_count = len(file_paths)
        
        if file_count <= 5 or total_size_mb < 10:  # ì‘ì€ ë°ì´í„°ì…‹
            return {
                "mode": "simple",
                "batch_size": 1,
                "queue_maxsize": 2
            }
        elif file_count <= 20 or total_size_mb < 100:  # ì¤‘ê°„ ë°ì´í„°ì…‹
            return {
                "mode": "hybrid", 
                "batch_size": 2,
                "queue_maxsize": 4
            }
        else:  # í° ë°ì´í„°ì…‹
            return {
                "mode": "hybrid",
                "batch_size": 3,
                "queue_maxsize": 6
            }
    except:
        return {
            "mode": "hybrid",
            "batch_size": 2,
            "queue_maxsize": 4
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ê³ ì„±ëŠ¥ ë³‘ë ¬ íšŒì˜ ìš”ì•½ ì‹œìŠ¤í…œ")
    parser.add_argument("input_pattern", help="ì…ë ¥ íŒŒì¼ íŒ¨í„´ (ì˜ˆ: 'data/*.jsonl')")
    parser.add_argument("-o", "--output", default="summary_result.jsonl",
                       help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("-m", "--model", default="Qwen/Qwen3-32B-AWQ",
                       help="ì‚¬ìš©í•  AWQ ëª¨ë¸")
    parser.add_argument("-b", "--batch-size", type=int,
                       help="íŒŒì¼ ë°°ì¹˜ í¬ê¸° (ìë™ ì„¤ì •)")
    parser.add_argument("-t", "--max-tokens", type=int, default=5000,
                       help="ì²­í¬ ìµœëŒ€ í† í° ìˆ˜")
    parser.add_argument("-s", "--stride", type=int, default=3500,
                       help="ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìŠ¤íŠ¸ë¼ì´ë“œ")
    parser.add_argument("--max-new-tokens", type=int, default=1024,
                       help="ìƒì„± ìµœëŒ€ í† í° ìˆ˜")
    parser.add_argument("--mode", choices=["simple", "hybrid"],
                       help="ì²˜ë¦¬ ëª¨ë“œ (ìë™ ì„ íƒ)")
    
    args = parser.parse_args()
    
    # ìµœì  ì„¤ì • ê²°ì •
    optimal = get_optimal_settings(args.input_pattern)
    
    batch_size = args.batch_size or optimal["batch_size"]
    mode = args.mode or optimal["mode"]
    
    logger.info(f"ìµœì í™”ëœ ì„¤ì •: ì…ë ¥={args.input_pattern}, ì¶œë ¥={args.output}, "
               f"ëª¨ë“œ={mode}, ë°°ì¹˜={batch_size}, ëª¨ë¸={args.model}")
    
    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ë° ì‹¤í–‰
    try:
        processor = ParallelSummaryProcessor(model_path=args.model)
        
        if mode == "simple":
            success = processor.process_simple_parallel(
                input_pattern=args.input_pattern,
                output_file=args.output,
                max_tokens=args.max_tokens,
                stride=args.stride,
                max_new_tokens=args.max_new_tokens
            )
        else:  # hybrid
            success = processor.process_hybrid_parallel(
                input_pattern=args.input_pattern,
                output_file=args.output,
                batch_size=batch_size,
                queue_maxsize=optimal["queue_maxsize"],
                max_tokens=args.max_tokens,
                stride=args.stride,
                max_new_tokens=args.max_new_tokens
            )
        
        if success:
            logger.info(f"ğŸ‰ ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ! ê²°ê³¼: {args.output}")
        else:
            logger.error("âŒ ì²˜ë¦¬ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        logger.warning("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    if len(os.sys.argv) == 1:
        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
        input_pattern = "label_0_output.jsonl"
        output_file = "summary_result.jsonl"
        
        if glob(input_pattern):
            logger.info("ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰")
            
            processor = ParallelSummaryProcessor()
            optimal = get_optimal_settings(input_pattern)
            
            if optimal["mode"] == "simple":
                success = processor.process_simple_parallel(input_pattern, output_file)
            else:
                success = processor.process_hybrid_parallel(
                    input_pattern=input_pattern,
                    output_file=output_file,
                    batch_size=optimal["batch_size"],
                    queue_maxsize=optimal["queue_maxsize"]
                )
            
            if success:
                logger.info(f"ğŸ‰ ì™„ë£Œ! ê²°ê³¼ í™•ì¸: {output_file}")
        else:
            logger.error(f"ê¸°ë³¸ íŒŒì¼({input_pattern})ì´ ì—†ìŠµë‹ˆë‹¤. ëª…ë ¹í–‰ ì¸ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            logger.info("ì‚¬ìš©ë²•: python script.py 'data/*.jsonl'")
    else:
        main()