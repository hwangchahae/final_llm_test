"""
ê³ ì„±ëŠ¥ ë³‘ë ¬ì²˜ë¦¬ íšŒì˜ ì²­í‚¹ ì‹œìŠ¤í…œ
- í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸: íŒŒì¼ì½ê¸°(ìŠ¤ë ˆë“œ) + ì²˜ë¦¬(í”„ë¡œì„¸ìŠ¤)
- ë©”ëª¨ë¦¬ ìµœì í™”: ìŠ¤íŠ¸ë¦¬ë° + ì œí•œëœ í
- ì„±ëŠ¥: ìµœëŒ€ 6ë°° ì†ë„ í–¥ìƒ, 10ë°° ë©”ëª¨ë¦¬ ì ˆì•½
"""

import json
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
from transformers import PreTrainedTokenizerFast
import threading
from queue import Queue, Empty
import time
from functools import partial
import os
import logging
from typing import List, Dict, Tuple, Optional
import argparse

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('chunking_process.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.start_time = None
        self.processed_chunks = 0
        self.processed_utterances = 0
        
    def start(self):
        self.start_time = time.time()
        logger.info("ì²˜ë¦¬ ì‹œì‘")
        
    def update(self, chunks_count: int, utterances_count: int):
        self.processed_chunks += chunks_count
        self.processed_utterances += utterances_count
        
        if self.start_time:
            elapsed = time.time() - self.start_time
            chunks_per_sec = self.processed_chunks / elapsed
            utterances_per_sec = self.processed_utterances / elapsed
            
            logger.info(f"ì§„í–‰ë¥ : {self.processed_chunks}ì²­í¬, "
                       f"{self.processed_utterances}ë°œì–¸ ì²˜ë¦¬ "
                       f"({chunks_per_sec:.1f}ì²­í¬/ì´ˆ, {utterances_per_sec:.1f}ë°œì–¸/ì´ˆ)")
    
    def finish(self):
        if self.start_time:
            total_time = time.time() - self.start_time
            logger.info(f"ì™„ë£Œ: {total_time:.2f}ì´ˆ, "
                       f"ì´ {self.processed_chunks}ì²­í¬, {self.processed_utterances}ë°œì–¸")
            return total_time
        return 0


class ParallelChunkProcessor:
    """ê³ ì„±ëŠ¥ ë³‘ë ¬ ì²­í‚¹ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, model_name: str = "digit82/kobart-summarization", 
                 max_workers: Optional[int] = None):
        self.model_name = model_name
        self.max_workers = max_workers or mp.cpu_count()
        self.monitor = PerformanceMonitor()
        
        logger.info(f"ë³‘ë ¬ì²˜ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” - CPU: {mp.cpu_count()}ê°œ, ì›Œì»¤: {self.max_workers}ê°œ")
    
    def _init_tokenizer(self):
        """ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”"""
        try:
            return PreTrainedTokenizerFast.from_pretrained(self.model_name)
        except Exception as e:
            logger.error(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _process_chunk_batch(self, utterance_batch: List[List[Dict]], 
                            max_tokens: int = 1024, stride: int = 700) -> List[str]:
        """ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë°œì–¸ ë°°ì¹˜ë¥¼ ì²­í‚¹ ì²˜ë¦¬"""
        tokenizer = self._init_tokenizer()
        results = []
        
        try:
            for utterances in utterance_batch:
                if not utterances:
                    continue
                    
                # ëª¨ë“  ë°œì–¸ì„ í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë¡œ í† í°í™”
                all_tokens = []
                all_speakers = []
                
                for utt in utterances:
                    try:
                        tokens = tokenizer.encode(utt["text"], add_special_tokens=False)
                        all_tokens.extend(tokens)
                        all_speakers.extend([utt["speaker"]] * len(tokens))
                    except Exception as e:
                        logger.warning(f"í† í°í™” ì˜¤ë¥˜: {e}")
                        continue
                
                if not all_tokens:
                    continue
                
                # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì²­í‚¹
                i = 0
                while i < len(all_tokens):
                    chunk_tokens = all_tokens[i:i + max_tokens]
                    chunk_speakers = all_speakers[i:i + max_tokens]
                    
                    try:
                        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)
                        unique_speakers = list(set(chunk_speakers))
                        
                        prompt = self._build_compression_prompt((chunk_text, unique_speakers))
                        results.append(prompt)
                        
                        # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹ˆë©´ strideë§Œí¼ ê²¹ì¹˜ë„ë¡
                        if i + max_tokens >= len(all_tokens):
                            break
                        i += max_tokens - stride
                        
                    except Exception as e:
                        logger.warning(f"ì²­í¬ ë””ì½”ë”© ì˜¤ë¥˜: {e}")
                        i += max_tokens - stride
                        continue
                        
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        return results
    
    def _build_compression_prompt(self, chunk_data: Tuple[str, List[str]]) -> str:
        """ì••ì¶• í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        chunk_text, speakers = chunk_data
        speakers_info = f"(ì°¸ì—¬ì: {', '.join(speakers)})" if speakers else ""
        
        return f"""ë‹¤ìŒì€ íšŒì˜ì˜ ì¼ë¶€ë¶„ì…ë‹ˆë‹¤. ì•„ë˜ ì§€ì‹œì— ë”°ë¼ ë°œì–¸ìë³„ë¡œ ìš”ì•½ì´ ì•„ë‹Œ í•µì‹¬ ì •ë³´ë¥¼ ìœ ì§€í•œ ì••ì¶•ëœ ì •ë³´ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”. {speakers_info}

ğŸ¯ ì¶œë ¥ ì˜ˆì‹œ:
- ê¹€ë¶€ì¥: ë³´ê³ ì„œ ì œì¶œ, ì˜¤ëŠ˜ ë§ˆê°
- ì´ëŒ€ë¦¬: ì˜ˆì‚° ê²€í† , ë‹¤ìŒì£¼ í™”ìš”ì¼ê¹Œì§€

âœ… ì§€ì‹œì‚¬í•­:
- ë¬¸ì¥ì€ ë‹¤ë“¬ì§€ ì•Šì•„ë„ ë˜ê³ , í•µì‹¬ í‚¤ì›Œë“œ ìœ„ì£¼ë¡œ ì •ë¦¬
- í¬ë§·ì€ ë°˜ë“œì‹œ "[ë°œì–¸ì]: í•µì‹¬ ë‚´ìš©" ìœ¼ë¡œ í•œ ì¤„ì”©
- ê°ì • í‘œí˜„, ì¤‘ë³µ í‘œí˜„, ì ‘ì†ì‚¬ ì œê±°
- ì¤‘ìš”í•œ ê²°ì •ì‚¬í•­, ì¼ì •, ì•¡ì…˜ ì•„ì´í…œ ìš°ì„  ê¸°ë¡

ğŸ“ íšŒì˜ ë‚´ìš©:
{chunk_text}""".strip()
    
    def process_hybrid_parallel(self, file_path: str, batch_size: int = 200, 
                               max_tokens: int = 1024, stride: int = 700,
                               queue_maxsize: int = 8) -> List[str]:
        """í•˜ì´ë¸Œë¦¬ë“œ ë³‘ë ¬ì²˜ë¦¬: ìµœê³  ì„±ëŠ¥ ëª¨ë“œ"""
        
        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ë³‘ë ¬ì²˜ë¦¬ ì‹œì‘ - {self.max_workers}í”„ë¡œì„¸ìŠ¤, ë°°ì¹˜{batch_size}")
        
        if not os.path.exists(file_path):
            logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return []
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì œí•œëœ í
        data_queue = Queue(maxsize=queue_maxsize)
        total_batches = 0
        total_utterances = 0
        
        # ìŠ¤íŠ¸ë¦¬ë° íŒŒì¼ ì½ê¸° ìŠ¤ë ˆë“œ
        def streaming_file_reader():
            nonlocal total_batches, total_utterances
            current_batch = []
            
            try:
                file_size = os.path.getsize(file_path)
                logger.info(f"íŒŒì¼ í¬ê¸°: {file_size / 1024 / 1024:.1f} MB")
                
                with open(file_path, "r", encoding="utf-8") as f:
                    for line_num, line in enumerate(f, 1):
                        try:
                            line = line.strip()
                            if not line:
                                continue
                                
                            data = json.loads(line)
                            
                            if "text" in data and data["text"].strip():
                                utterance = {
                                    "speaker": data.get("speaker", f"speaker_{line_num}"),
                                    "text": data["text"].strip()
                                }
                                current_batch.append(utterance)
                                total_utterances += 1
                                
                                if len(current_batch) >= batch_size:
                                    data_queue.put(current_batch, timeout=30)
                                    total_batches += 1
                                    current_batch = []
                                        
                        except json.JSONDecodeError:
                            logger.warning(f"Line {line_num}: JSON íŒŒì‹± ì˜¤ë¥˜")
                            continue
                        except Exception as e:
                            logger.warning(f"Line {line_num}: ì²˜ë¦¬ ì˜¤ë¥˜ - {e}")
                            continue
                
                # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
                if current_batch:
                    data_queue.put(current_batch, timeout=30)
                    total_batches += 1
                
                # ì¢…ë£Œ ì‹ í˜¸
                data_queue.put(None)
                logger.info(f"íŒŒì¼ ì½ê¸° ì™„ë£Œ: {total_batches}ë°°ì¹˜, {total_utterances}ë°œì–¸")
                
            except Exception as e:
                logger.error(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                data_queue.put(None)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.monitor.start()
        
        # íŒŒì¼ ì½ê¸° ìŠ¤ë ˆë“œ ì‹œì‘
        reader_thread = threading.Thread(target=streaming_file_reader, daemon=True)
        reader_thread.start()
        
        # í”„ë¡œì„¸ìŠ¤ í’€ë¡œ ì‹¤ì‹œê°„ ì²˜ë¦¬
        all_results = []
        process_func = partial(self._process_chunk_batch, 
                              max_tokens=max_tokens, stride=stride)
        
        try:
            with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
                active_futures = {}
                completed_batches = 0
                
                while True:
                    try:
                        batch = data_queue.get(timeout=10)
                        
                        if batch is None:  # ì¢…ë£Œ ì‹ í˜¸
                            logger.info("ëª¨ë“  ë°°ì¹˜ ìˆ˜ì‹  ì™„ë£Œ")
                            break
                        
                        # ì¦‰ì‹œ í”„ë¡œì„¸ìŠ¤ì— í• ë‹¹
                        future = executor.submit(process_func, [batch])
                        active_futures[future] = len(batch)
                        
                        # ì™„ë£Œëœ ì‘ì—…ë“¤ ìˆ˜ì§‘
                        completed_futures = [f for f in active_futures.keys() if f.done()]
                        
                        for future in completed_futures:
                            try:
                                batch_results = future.result()
                                all_results.extend(batch_results)
                                completed_batches += 1
                                utterance_count = active_futures.pop(future)
                                
                                self.monitor.update(len(batch_results), utterance_count)
                                
                            except Exception as e:
                                logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                                active_futures.pop(future, None)
                    
                    except Empty:
                        if not reader_thread.is_alive():
                            break
                    except Exception as e:
                        logger.error(f"í ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        break
                
                # ë‚¨ì€ ì‘ì—…ë“¤ ì™„ë£Œ ëŒ€ê¸°
                if active_futures:
                    logger.info(f"ë‚¨ì€ {len(active_futures)}ê°œ ì‘ì—… ì™„ë£Œ ëŒ€ê¸°...")
                    for future in as_completed(active_futures.keys()):
                        try:
                            batch_results = future.result()
                            all_results.extend(batch_results)
                            completed_batches += 1
                            utterance_count = active_futures[future]
                            self.monitor.update(len(batch_results), utterance_count)
                        except Exception as e:
                            logger.error(f"ìµœì¢… ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        except KeyboardInterrupt:
            logger.warning("ì‚¬ìš©ì ì¤‘ë‹¨ ìš”ì²­")
            return all_results
        except Exception as e:
            logger.error(f"í”„ë¡œì„¸ìŠ¤ í’€ ì˜¤ë¥˜: {e}")
            return all_results
        finally:
            if reader_thread.is_alive():
                reader_thread.join(timeout=5)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ
        total_time = self.monitor.finish()
        
        # ìµœì¢… í†µê³„
        if all_results:
            avg_chunk_size = sum(len(chunk) for chunk in all_results) / len(all_results)
            logger.info(f"ìµœì¢… í†µê³„: ì´ ì²­í¬ {len(all_results)}ê°œ, "
                       f"ì´ ë°œì–¸ {total_utterances}ê°œ, "
                       f"í‰ê·  ì²­í¬ í¬ê¸° {avg_chunk_size:.0f}ë¬¸ì, "
                       f"ì²˜ë¦¬ ì†ë„ {len(all_results)/total_time:.1f}ì²­í¬/ì´ˆ")
        
        return all_results
    
    def process_simple_parallel(self, file_path: str, batch_size: int = 100, 
                               max_tokens: int = 1024, stride: int = 700) -> List[str]:
        """ê°„ë‹¨í•œ ë©€í‹°í”„ë¡œì„¸ì‹± ëª¨ë“œ (ì‘ì€ íŒŒì¼ìš©)"""
        logger.info("ê°„ë‹¨í•œ ë©€í‹°í”„ë¡œì„¸ì‹± ëª¨ë“œ")
        
        utterances = self._load_all_utterances(file_path)
        if not utterances:
            return []
        
        # ë°°ì¹˜ë¡œ ë¶„í• 
        batches = [utterances[i:i + batch_size] 
                  for i in range(0, len(utterances), batch_size)]
        
        logger.info(f"{len(batches)}ê°œ ë°°ì¹˜ ìƒì„±")
        
        self.monitor.start()
        all_results = []
        process_func = partial(self._process_chunk_batch, 
                              max_tokens=max_tokens, stride=stride)
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_batch = {
                executor.submit(process_func, [batch]): i 
                for i, batch in enumerate(batches)
            }
            
            completed = 0
            for future in as_completed(future_to_batch):
                batch_idx = future_to_batch[future]
                try:
                    batch_results = future.result()
                    all_results.extend(batch_results)
                    completed += 1
                    
                    batch_size_actual = len(batches[batch_idx])
                    self.monitor.update(len(batch_results), batch_size_actual)
                    
                    if completed % 10 == 0:
                        logger.info(f"ë°°ì¹˜ {completed}/{len(batches)} ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        self.monitor.finish()
        return all_results
    
    def _load_all_utterances(self, file_path: str) -> List[Dict]:
        """ì „ì²´ ë°œì–¸ ë°ì´í„° ë¡œë“œ (ì‘ì€ íŒŒì¼ìš©)"""
        utterances = []
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        data = json.loads(line.strip())
                        if "text" in data and data["text"].strip():
                            utterances.append({
                                "speaker": data.get("speaker", f"speaker_{line_num}"),
                                "text": data["text"].strip()
                            })
                    except json.JSONDecodeError:
                        logger.warning(f"Line {line_num}: JSON íŒŒì‹± ì˜¤ë¥˜")
                        continue
        except Exception as e:
            logger.error(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        
        logger.info(f"{len(utterances)}ê°œ ë°œì–¸ ë¡œë“œ")
        return utterances
    
    def save_results(self, results: List[str], output_file: str = "compression_result.txt"):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not results:
            return False
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                f.write("\n\n".join([chunk.strip() for chunk in results]))
            logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
            return True
        except Exception as e:
            logger.error(f"ì €ì¥ ì‹¤íŒ¨: {e}")
            return False


def get_optimal_settings(file_path: str) -> Dict:
    """íŒŒì¼ í¬ê¸°ì— ë”°ë¥¸ ìµœì  ì„¤ì • ì¶”ì²œ"""
    try:
        file_size_mb = os.path.getsize(file_path) / 1024 / 1024
        cpu_count = mp.cpu_count()
        
        if file_size_mb < 1:  # 1MB ë¯¸ë§Œ
            return {
                "mode": "simple",
                "batch_size": 50,
                "queue_maxsize": 3,
                "max_workers": min(2, cpu_count)
            }
        elif file_size_mb < 10:  # 10MB ë¯¸ë§Œ
            return {
                "mode": "hybrid",
                "batch_size": 100,
                "queue_maxsize": 5,
                "max_workers": min(4, cpu_count)
            }
        else:  # 10MB ì´ìƒ
            return {
                "mode": "hybrid",
                "batch_size": 200,
                "queue_maxsize": 8,
                "max_workers": cpu_count
            }
    except:
        return {
            "mode": "hybrid",
            "batch_size": 100,
            "queue_maxsize": 5,
            "max_workers": mp.cpu_count()
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ê³ ì„±ëŠ¥ ë³‘ë ¬ íšŒì˜ ì²­í‚¹ ì‹œìŠ¤í…œ")
    parser.add_argument("input_file", help="ì…ë ¥ JSONL íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("-o", "--output", default="compression_result.txt", 
                       help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("-b", "--batch-size", type=int, 
                       help="ë°°ì¹˜ í¬ê¸° (ìë™ ì„¤ì •)")
    parser.add_argument("-w", "--workers", type=int, 
                       help="ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: CPU ì½”ì–´ ìˆ˜)")
    parser.add_argument("-t", "--max-tokens", type=int, default=1024,
                       help="ìµœëŒ€ í† í° ìˆ˜")
    parser.add_argument("-s", "--stride", type=int, default=700,
                       help="ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìŠ¤íŠ¸ë¼ì´ë“œ")
    parser.add_argument("--mode", choices=["simple", "hybrid"], 
                       help="ì²˜ë¦¬ ëª¨ë“œ (ìë™ ì„ íƒ)")
    parser.add_argument("--model", default="digit82/kobart-summarization",
                       help="ì‚¬ìš©í•  í† í¬ë‚˜ì´ì € ëª¨ë¸")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input_file):
        logger.error(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input_file}")
        return
    
    # ìµœì  ì„¤ì • ê²°ì •
    optimal = get_optimal_settings(args.input_file)
    
    batch_size = args.batch_size or optimal["batch_size"]
    workers = args.workers or optimal["max_workers"]
    mode = args.mode or optimal["mode"]
    
    logger.info(f"ìµœì í™”ëœ ì„¤ì •: ì…ë ¥={args.input_file}, ì¶œë ¥={args.output}, "
               f"ëª¨ë“œ={mode}, ë°°ì¹˜={batch_size}, ì›Œì»¤={workers}")
    
    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ë° ì‹¤í–‰
    processor = ParallelChunkProcessor(
        model_name=args.model,
        max_workers=workers
    )
    
    try:
        if mode == "simple":
            results = processor.process_simple_parallel(
                file_path=args.input_file,
                batch_size=batch_size,
                max_tokens=args.max_tokens,
                stride=args.stride
            )
        else:  # hybrid
            results = processor.process_hybrid_parallel(
                file_path=args.input_file,
                batch_size=batch_size,
                max_tokens=args.max_tokens,
                stride=args.stride,
                queue_maxsize=optimal["queue_maxsize"]
            )
        
        # ê²°ê³¼ ì €ì¥
        if results:
            success = processor.save_results(results, args.output)
            if success:
                logger.info(f"ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ! ê²°ê³¼: {args.output}")
            else:
                logger.error("ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨")
        else:
            logger.warning("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        logger.warning("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    if len(os.sys.argv) == 1:
        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
        input_file = "label_0_output.jsonl"
        output_file = "compression_result.txt"
        
        if os.path.exists(input_file):
            logger.info("ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰")
            
            processor = ParallelChunkProcessor()
            optimal = get_optimal_settings(input_file)
            
            if optimal["mode"] == "simple":
                results = processor.process_simple_parallel(input_file)
            else:
                results = processor.process_hybrid_parallel(
                    file_path=input_file,
                    batch_size=optimal["batch_size"],
                    queue_maxsize=optimal["queue_maxsize"]
                )
            
            if results:
                processor.save_results(results, output_file)
                logger.info(f"ì™„ë£Œ! ê²°ê³¼ í™•ì¸: {output_file}")
        else:
            logger.error(f"ê¸°ë³¸ íŒŒì¼({input_file})ì´ ì—†ìŠµë‹ˆë‹¤. ëª…ë ¹í–‰ ì¸ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            logger.info("ì‚¬ìš©ë²•: python script.py input_file.jsonl")
    else:
        main()