# !pip install vllm

import os, json, re
from glob import glob
from tqdm import tqdm
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import torch
from datetime import datetime

# âœ… 1. VLLM ëª¨ë¸ ë¡œë“œ
model_path = "Qwen/Qwen3-32B-AWQ"

# VLLM ì—”ì§„ ì´ˆê¸°í™”
llm = LLM(
    model=model_path,
    quantization="awq_marlin",  # ë” ë¹ ë¥¸ awq_marlin ì‚¬ìš©
    tensor_parallel_size=1,
    max_model_len=16384,
    gpu_memory_utilization=0.9,
    trust_remote_code=True,
    enforce_eager=False,
)

# í† í¬ë‚˜ì´ì €ëŠ” ë³„ë„ë¡œ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True
)

# ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
sampling_params = SamplingParams(
    temperature=0.1,
    max_tokens=2048,
    stop=None,  # ëª¨ë¸ì´ ìì—°ìŠ¤ëŸ½ê²Œ ëë‚´ë„ë¡ í•¨
    skip_special_tokens=True,  # EOS í† í°ì—ì„œ ìë™ ì¢…ë£Œ
)

def clean_text(text):
    if not text:
        return ""
    
    # íŠ¹ì • íƒœê·¸ë“¤ë§Œ ì œê±°
    text = re.sub(r'\[TGT\]', '', text)
    text = re.sub(r'\[/TGT\]', '', text)

    # ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# âœ… 2. JSONL ë¡œë“œ 
def load_jsonl(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return [
            {
                "speaker": json.loads(line).get("speaker"),
                "text": clean_text(json.loads(line).get("text"))  # íƒœê·¸ ì œê±°
            }
            for line in f if "text" in json.loads(line)
        ]

# âœ… 3. ì²­í¬ ë¶„í•  
def chunk_text(utterances, max_tokens=5000, stride=512):
    input_ids = []
    metadata = []

    for utt in utterances:
        tokens = tokenizer.encode(utt["text"], add_special_tokens=False)
        input_ids.extend(tokens)
        metadata.extend([utt["speaker"]] * len(tokens))

    chunks = []
    speakers_per_chunk = []

    i = 0
    while i < len(input_ids):
        chunk_ids = input_ids[i:i + max_tokens]
        chunk_speakers = metadata[i:i + max_tokens]

        chunk_text = tokenizer.decode(chunk_ids)
        unique_speakers = list(set(chunk_speakers))

        chunks.append(chunk_text)
        speakers_per_chunk.append(unique_speakers)

        i += max_tokens - stride

    return list(zip(chunks, speakers_per_chunk))

# âœ… 4. íŒŒì¼ ë‚ ì§œ ì¶”ì¶œ í•¨ìˆ˜ (ê°„ë‹¨í™”)
def get_file_date(file_path):
    """íŒŒì¼ëª… ìš°ì„  â†’ ë©”íƒ€ë°ì´í„° ì°¨ì„ """
    # 1. íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
    filename = os.path.basename(file_path)
    date_match = re.search(r'(\d{6})', filename)
    if date_match:
        date_str = date_match.group(1)
        try:
            year = "20" + date_str[:2]
            month = date_str[2:4] 
            day = date_str[4:6]
            return f"{year}-{month}-{day}"
        except:
            pass
    
    # 2. ë©”íƒ€ë°ì´í„°ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
    try:
        mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
        return mtime.strftime("%Y-%m-%d")
    except:
        pass
    
    # 3. í˜„ì¬ ë‚ ì§œ
    return datetime.now().strftime("%Y-%m-%d")

# âœ… 5. ìš”ì•½ ìƒì„± í•¨ìˆ˜ (ì²­í¬ë³„ ì²˜ë¦¬)
def generate(prompt, chunk_index):
    outputs = llm.generate([prompt], sampling_params)
    result = outputs[0].outputs[0].text.strip()
    
    if chunk_index == 0:
        # ì²« ë²ˆì§¸ ì²­í¬: ì „ì²´ êµ¬ì¡° ë°˜í™˜
        match = re.search(r"(### ìš”ì•½[\s\S]*)", result)
        return match.group(1).strip() if match else result
    else:
        # ë‘ ë²ˆì§¸ ì²­í¬ë¶€í„°: ë‘ ë²ˆì§¸ ### ìš”ì•½ë¶€í„° ëê¹Œì§€
        summary_matches = list(re.finditer(r"### ìš”ì•½", result))
        if len(summary_matches) >= 2:
            return result[summary_matches[1].start():].strip()
        else:
            return result

# âœ… 6. ì „ì²´ ì²˜ë¦¬ (ì¤‘ë³µ ì œê±° ë° ë‚ ì§œ ê°œì„ )
def create_training_dataset(input_dir_pattern, output_jsonl):
    file_paths = glob(input_dir_pattern)
    with open(output_jsonl, "w", encoding="utf-8") as f_out:
        for file_path in tqdm(file_paths, desc="ğŸ“‚ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ì§„í–‰"):
            print(f"\nğŸ“ ì²˜ë¦¬ ì¤‘: {file_path}")
            
            # íŒŒì¼ ë‚ ì§œ ì¶”ì¶œ
            file_date = get_file_date(file_path)
            print(f"ğŸ“… ê¸°ì¤€ ë‚ ì§œ: {file_date}")
            
            utterances = load_jsonl(file_path)
            chunks = chunk_text(utterances)

            summary_accum = ""
            for idx, (chunk, speakers) in enumerate(tqdm(chunks, desc=f"ğŸ§© ì²­í¬ ì²˜ë¦¬ ({os.path.basename(file_path)})", leave=False)):
                participants_str = ", ".join(speakers)

                # ì²« ë²ˆì§¸ ì²­í¬ëŠ” ì´ì „ ìš”ì•½ ì œì™¸
                if idx == 0:
                    prompt = f"""
ë‹¤ìŒì€ íšŒì˜ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.

[ì°¸ì—¬ì]
{participants_str}

[íšŒì˜ ì²­í¬]
{chunk}

1. ì´ íšŒì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜.
2. ì•ˆê±´ì´ ìˆë‹¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•´ì¤˜.
3. ê° ì•ˆê±´ì— ëŒ€í•´ í•„ìš”í•œ ì‘ì—…ë“¤ì„ ë¶„í•´í•´ì¤˜. (ëˆ„ê°€, ë¬´ì—‡ì„, ì–¸ì œê¹Œì§€)

**ì¤‘ìš”**: ì—…ë¬´ ë¶„í•´ì—ì„œ ë§ˆê°ì¼ì€ íšŒì˜ ë‚ ì§œ({file_date}) ê¸°ì¤€ìœ¼ë¡œ 1ì£¼ì¼~2ì£¼ì¼ í›„ì˜ í˜„ì‹¤ì ì¸ ë‚ ì§œë¥¼ ë‹¤ì–‘í•˜ê²Œ ì‚¬ìš©í•´ì¤˜. ëª¨ë“  ì—…ë¬´ê°€ ê°™ì€ ë‚ ì§œë©´ ì•ˆ ë¨.

ê²°ê³¼ëŠ” ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¤˜:

### ìš”ì•½
- ...

### ì•ˆê±´
1. ì œëª©: ...
   - ì„¸ë¶€ ì„¤ëª…
   - ê´€ë ¨ ë°œì–¸ì

### ì—…ë¬´ ë¶„í•´
- [ì—…ë¬´]: ë‹´ë‹¹ì, ë§ˆê°ì¼, ê´€ë ¨ ì•ˆê±´
"""
                else:
                    prompt = f"""
ë‹¤ìŒì€ íšŒì˜ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.

[ì°¸ì—¬ì]
{participants_str}

[íšŒì˜ ì²­í¬]
{chunk}

[ì´ì „ê¹Œì§€ì˜ ìš”ì•½]
{summary_accum}

1. ì´ íšŒì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜.
2. ì•ˆê±´ì´ ìˆë‹¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•´ì¤˜.
3. ê° ì•ˆê±´ì— ëŒ€í•´ í•„ìš”í•œ ì‘ì—…ë“¤ì„ ë¶„í•´í•´ì¤˜. (ëˆ„ê°€, ë¬´ì—‡ì„, ì–¸ì œê¹Œì§€)

**ì¤‘ìš”**: ì—…ë¬´ ë¶„í•´ì—ì„œ ë§ˆê°ì¼ì€ íšŒì˜ ë‚ ì§œ({file_date}) ê¸°ì¤€ìœ¼ë¡œ 1ì£¼ì¼~2ì£¼ì¼ í›„ì˜ í˜„ì‹¤ì ì¸ ë‚ ì§œë¥¼ ë‹¤ì–‘í•˜ê²Œ ì‚¬ìš©í•´ì¤˜. ëª¨ë“  ì—…ë¬´ê°€ ê°™ì€ ë‚ ì§œë©´ ì•ˆ ë¨.

ê²°ê³¼ëŠ” ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¤˜:

### ìš”ì•½
- ...

### ì•ˆê±´
1. ì œëª©: ...
   - ì„¸ë¶€ ì„¤ëª…
   - ê´€ë ¨ ë°œì–¸ì

### ì—…ë¬´ ë¶„í•´
- [ì—…ë¬´]: ë‹´ë‹¹ì, ë§ˆê°ì¼, ê´€ë ¨ ì•ˆê±´
"""
                
                response = generate(prompt, idx)
                json.dump({
                    "file": os.path.basename(file_path),
                    "chunk_index": idx,
                    "file_date": file_date,
                    "response": response
                }, f_out, ensure_ascii=False)
                f_out.write("\n")
                summary_accum = response + "\n"

def save_final_result_as_txt(output_file, txt_file):
    try:
        with open(output_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
        
        if not lines:
            print("âŒ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return
        
        # ë§ˆì§€ë§‰ ê²°ê³¼ íŒŒì‹±
        data = json.loads(lines[-1].strip())
        response = data['response']
        file_date = data.get('file_date', 'N/A')
        
        # ë‚´ìš© êµ¬ì„±
        content = []
        content.append("=" * 80)
        content.append(f"ğŸ“‹ ìµœì¢… íšŒì˜ ìš”ì•½ ê²°ê³¼ (ì²­í¬ {data['chunk_index']})")
        content.append(f"ğŸ“… íšŒì˜ ë‚ ì§œ: {file_date}")
        content.append("=" * 80)
        content.append("")
        
        # ì„¹ì…˜ë³„ í¬ë§·íŒ…
        sections = response.split('### ')
        for section in sections:
            if not section.strip():
                continue
                
            section = section.strip()
            
            if section.startswith('ìš”ì•½'):
                content.append(f"ğŸ¯ ìš”ì•½")
                content.append("-" * 60)
                text_content = section.split('\n', 1)[1] if '\n' in section else ""
                for line in text_content.split('\n'):
                    if line.strip():
                        content.append(f"  {line.strip()}")
                content.append("")
                        
            elif section.startswith('ì•ˆê±´'):
                content.append(f"ğŸ“Œ ì•ˆê±´")
                content.append("-" * 60)
                text_content = section.split('\n', 1)[1] if '\n' in section else ""
                for line in text_content.split('\n'):
                    if line.strip():
                        if line.strip().startswith(('1.', '2.', '3.', '4.', '5.')):
                            content.append(f"\n  ğŸ“ {line.strip()}")
                        else:
                            content.append(f"     {line.strip()}")
                content.append("")
                            
            elif section.startswith('ì—…ë¬´ ë¶„í•´'):
                content.append(f"âš¡ ì—…ë¬´ ë¶„í•´")
                content.append("-" * 60)
                text_content = section.split('\n', 1)[1] if '\n' in section else ""
                for line in text_content.split('\n'):
                    if line.strip() and line.strip().startswith('-'):
                        content.append(f"  âœ… {line.strip()[1:].strip()}")
                content.append("")
        
        content.append("=" * 80)
        
        # íŒŒì¼ ì €ì¥
        with open(txt_file, "w", encoding="utf-8") as f:
            f.write("\n".join(content))
        
        print(f"âœ… ê²°ê³¼ê°€ {txt_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")

# âœ… 8. ì‹¤í–‰
if __name__ == "__main__":
    input_file = "/workspace/250724_data2_input_lee.jsonl"
    output_file = "/workspace/250724_data2_output_lee.jsonl"
    txt_file = "/workspace/250727_data2_final_result_lee.txt"
    
    create_training_dataset(input_file, output_file)
    save_final_result_as_txt(output_file, txt_file)