# !pip install git+https://github.com/casper-hansen/AutoAWQ.git

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°
from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast, AutoTokenizer
import os, json, re, yaml
from glob import glob
from tqdm import tqdm                      # ì§„í–‰ ìƒí™© ë³´ê¸°ìš©
from awq import AutoAWQForCausalLM        # Qwen AWQ ëª¨ë¸ ë¡œë”
import torch

# âœ… 1. ëª¨ë¸ ë¡œë“œ
model_path = "Qwen/Qwen3-32B-AWQ"

# Qwenìš© tokenizer ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# Qwen ì–‘ìí™” ëª¨ë¸ ë¡œë“œ
model = AutoAWQForCausalLM.from_quantized(
    model_path,
    fuse_layers=True,
    trust_remote_code=True,
    safetensors=True,
    device_map="auto",
    offload_folder="./offload"
)

# ì••ì¶• ëª¨ë¸ KoBART ë¡œë“œ
compress_model_name = "digit82/kobart-summarization"
compress_tokenizer = PreTrainedTokenizerFast.from_pretrained(compress_model_name)
compress_model = BartForConditionalGeneration.from_pretrained(compress_model_name).to("cuda")


def compress_text_kobart(text, max_input_tokens = compress_tokenizer.model_max_length):

    prompt = """
    ë‹¤ìŒì€ íšŒì˜ ê¸°ë¡ì˜ ì¼ë¶€ë¶„ì´ì•¼. íšŒì˜ì •ë³´ ì••ì¶• ì „ë¬¸ê°€ë¡œì„œ
1. íšŒì˜ í…ìŠ¤íŠ¸ì˜ ë°œì–¸ìì™€ ë°œì–¸ë‚´ìš©ì„ ë³´ê³  í•µì‹¬ ì˜ë¯¸ë¥¼ ë³´ì¡´ì„ ìµœìš°ì„ í•˜ë©´ì„œ, ì •ë³´ ì†Œì‹¤ì´ ì—†ê²Œ ë¬¸ì¥ê³¼ ë‹¨ì–´ì˜ ìˆ˜ë¥¼ ê°€ëŠ¥í•œ í•œ ì¤„ì—¬ ì••ì¶•í•´ì£¼ì„¸ìš”. 
2. ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´, ë°˜ë³µ, ì˜ˆì‹œëŠ” ìƒëµí•˜ê³ , ë¬¸ì¥ ê°„ ì¤‘ë³µë„ ìµœëŒ€í•œ ì œê±°í•´ì£¼ì„¸ìš”.
3. ì••ì¶•ëœ ë‚´ìš©ì„ ì •ë¦¬í•˜ê³  'ë°œì–¸ì': 'ë°œì–¸ë‚´ìš©' í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. 
  """
    prompt_ids = compress_tokenizer.encode(prompt, add_special_tokens=False)
    prompt_len = len(prompt_ids)

    allowed_text_len = max_input_tokens - prompt_len
    text_ids = compress_tokenizer.encode(text, add_special_tokens=False)[:allowed_text_len]


    # í”„ë¡¬í”„íŠ¸ + ë³¸ë¬¸ ê²°í•©
    full_ids = prompt_ids + text_ids
    input_ids = torch.tensor([full_ids]).to("cuda")

    # ìƒì„±
    try:
        summary_ids = compress_model.generate(input_ids, max_length=512, num_beams=4, early_stopping=True)
        output_text = compress_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        return output_text.strip()
    except Exception as e:
        return "[ì••ì¶• ì‹¤íŒ¨: " + str(e) + "]"
    

def sliding_window_compress(text, max_input_tokens=compress_tokenizer.model_max_length, stride=700, return_list=False):
    input_ids = compress_tokenizer.encode(text, add_special_tokens=False)
    compressed_chunks = []
    i = 0
    while i < len(input_ids):
        chunk_ids = input_ids[i:i + max_input_tokens]
        chunk_text = compress_tokenizer.decode(chunk_ids, skip_special_tokens=True)
        compressed = compress_text_kobart(chunk_text, max_input_tokens)
        compressed_chunks.append(compressed)
        i += stride
    return compressed_chunks if return_list else "\n".join(compressed_chunks)


def create_training_dataset(input_dir_pattern, output_jsonl):

    file_paths = glob(input_dir_pattern)
    with open(output_jsonl, "w", encoding="utf-8") as f_out:
        for file_path in tqdm(file_paths, desc="ğŸ“‚ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ì§„í–‰"):
            print(f"\nğŸ“ ì²˜ë¦¬ ì¤‘: {file_path}")
            utterances = load_jsonl(file_path)

            #  1. [speaker] ë°œì–¸ìœ¼ë¡œ í…ìŠ¤íŠ¸ êµ¬ì„±
            full_text = "\n".join(f"[{utt['speaker']}] {utt['text']}" for utt in utterances)

            #  2. ìŠ¬ë¼ì´ë”© ì••ì¶•
            compressed_chunks = sliding_window_compress(full_text)

            summary_accum = ""
            for idx, chunk in enumerate(tqdm(compressed_chunks, desc=f"ğŸ§© ì••ì¶• ì²­í¬ ì²˜ë¦¬ ({os.path.basename(file_path)})", leave=False)):
                prompt = f"""
ë‹¤ìŒì€ íšŒì˜ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.

[íšŒì˜ ë‚´ìš©]
{chunk}

[ì´ì „ê¹Œì§€ì˜ ì••ì¶•ë‚´ìš©]
{summary_accum}

1. ì´ íšŒì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜.
2. ì•ˆê±´ì´ ìˆë‹¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•´ì¤˜.
3. ê° ì•ˆê±´ì— ëŒ€í•´ í•„ìš”í•œ ì‘ì—…ë“¤ì„ ë¶„í•´í•´ì¤˜. (ëˆ„ê°€, ë¬´ì—‡ì„, ì–¸ì œê¹Œì§€)

ê²°ê³¼ëŠ” ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¤˜:

### ìš”ì•½
- ...

### ì•ˆê±´
1. ì œëª©: ...
   - ì„¸ë¶€ ì„¤ëª…
   - ê´€ë ¨ ë°œì–¸ì

### ì—…ë¬´ ë¶„í•´
- [ì—…ë¬´]: ë‹´ë‹¹ì, ë§ˆê°ì¼, ê´€ë ¨ ì•ˆê±´
"""
                response = qwen_generate(prompt)
                json.dump({
                    "file": os.path.basename(file_path),
                    "chunk_index": idx,
                    "response": response
                }, f_out, ensure_ascii=False)
                f_out.write("\n")
                summary_accum += response + "\n"


# âœ… 2. JSONL ë¡œë“œ (speaker í¬í•¨)
def load_jsonl(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return [
            {
                "speaker": json.loads(line).get("speaker"),
                "text": json.loads(line).get("text")
            }
            for line in f if "text" in json.loads(line)
        ]


# âœ… 3. ìš”ì•½ ìƒì„± í•¨ìˆ˜
def qwen_generate(prompt, max_new_tokens=2048):
    enc = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        return_attention_mask=True
    ).to("cuda")

    output = model.generate(
        input_ids=enc.input_ids,
        attention_mask=enc.attention_mask,
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.1
    )

    result = tokenizer.decode(output[0], skip_special_tokens=True)

    # í”„ë¡¬í”„íŠ¸ ì œê±°
    if prompt.strip() in result:
        result = result.replace(prompt.strip(), "", 1).strip()

    # 'ìš”ì•½' ì´í›„ ë‚´ìš©ë§Œ ì¶”ì¶œ
    match = re.search(r"(### ìš”ì•½[\s\S]*)", result)
    return match.group(1).strip() if match else result


# âœ… 4. ì „ì²´ ì²˜ë¦¬
def create_qwen_summary(
    input_jsonl_path: str,
    output_txt_path: str,
    max_input_tokens=compress_tokenizer.model_max_length,
    stride=700
):
    """ì „ì²´ ì••ì¶• + Qwen ìš”ì•½ ì²˜ë¦¬ í›„ txt ì €ì¥"""

    utterances = load_jsonl(input_jsonl_path)

    # âœ… [ë°œì–¸ì] í˜•ì‹ìœ¼ë¡œ ì´ì–´ë¶™ì´ê¸°
    full_text = "\n".join(f"[{utt['speaker']}] {utt['text']}" for utt in utterances)

    # âœ… ìŠ¬ë¼ì´ë”© ì••ì¶•
    compressed_text = sliding_window_compress(full_text, max_input_tokens, stride)


    qwen_prompt = f"""
ë‹¤ìŒì€ íšŒì˜ ë‚´ìš© ì¼ë¶€ì…ë‹ˆë‹¤:

[íšŒì˜ ë‚´ìš©]
{compressed_text}

1. ì´ íšŒì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜.
2. ì•ˆê±´ì´ ìˆë‹¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•´ì¤˜.
3. ê° ì•ˆê±´ì— ëŒ€í•´ í•„ìš”í•œ ì‘ì—…ë“¤ì„ ë¶„í•´í•´ì¤˜. (ëˆ„ê°€, ë¬´ì—‡ì„, ì–¸ì œê¹Œì§€)

ê²°ê³¼ëŠ” ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¤˜:

### ìš”ì•½
- ...

### ì•ˆê±´
1. ì œëª©: ...
   - ì„¸ë¶€ ì„¤ëª…
   - ê´€ë ¨ ë°œì–¸ì

### ì—…ë¬´ ë¶„í•´
- [ì—…ë¬´]: ë‹´ë‹¹ì, ë§ˆê°ì¼, ê´€ë ¨ ì•ˆê±´
"""
    # âœ… Qwenìœ¼ë¡œ ìš”ì•½ ìƒì„±
    response = qwen_generate(qwen_prompt)

    # âœ… ê²°ê³¼ ì €ì¥
    with open(output_txt_path, "w", encoding="utf-8") as f:
        f.write(f"ğŸ“„ FILE: {os.path.basename(input_jsonl_path)}\n\n")
        f.write(response)

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_txt_path}")


# âœ… 6. ì‹¤í–‰ ì§„ì…ì 
if __name__ == "__main__":
    create_qwen_summary(
        input_jsonl_path="250724_data2_input_sk.jsonl",
        output_txt_path="250724_data2_output_sk.txt"
    )