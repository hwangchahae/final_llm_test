# !pip install git+https://github.com/casper-hansen/AutoAWQ.git

import os, json, re
from glob import glob
from tqdm import tqdm
from transformers import AutoTokenizer
from awq import AutoAWQForCausalLM
import torch

# âœ… 1. ëª¨ë¸ ë¡œë“œ
model_path = "Qwen/Qwen3-32B-AWQ"

tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True
)

model = AutoAWQForCausalLM.from_quantized(
    model_path,
    fuse_layers=True,
    trust_remote_code=True,
    safetensors=True,
    device_map="auto",
    offload_folder="./offload"
)


# âœ… 2. JSONL ë¡œë“œ (speaker í¬í•¨)
def load_jsonl(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return [
            {
                "speaker": json.loads(line).get("speaker", "unknown"),
                "text": json.loads(line).get("text", "")
            }
            for line in f if "text" in json.loads(line)
        ]


# âœ… 3. ì²­í¬ ë¶„í•  (speaker ì¶”ì )
def chunk_text(utterances, max_tokens=5000, stride=512):
    input_ids = []
    metadata = []  # ê° tokenì´ ëˆ„êµ¬ ë°œì–¸ì¸ì§€ ì¶”ì 

    for utt in utterances:
        tokens = tokenizer.encode(utt["text"], add_special_tokens=False)
        input_ids.extend(tokens)
        metadata.extend([utt["speaker"]] * len(tokens))

    chunks = []
    speakers_per_chunk = []

    i = 0
    while i < len(input_ids):
        chunk_ids = input_ids[i:i + max_tokens]
        chunk_speakers = metadata[i:i + max_tokens]

        chunk_text = tokenizer.decode(chunk_ids)
        unique_speakers = list(set(chunk_speakers))

        chunks.append(chunk_text)
        speakers_per_chunk.append(unique_speakers)

        i += max_tokens - stride

    return list(zip(chunks, speakers_per_chunk))


# âœ… 4. ìš”ì•½ ìƒì„± í•¨ìˆ˜
def generate(prompt, max_new_tokens=1024):
    enc = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        return_attention_mask=True
    ).to("cuda")

    output = model.generate(
        input_ids=enc.input_ids,
        attention_mask=enc.attention_mask,
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
        temperature=0.1
    )

    result = tokenizer.decode(output[0], skip_special_tokens=True)

    # í”„ë¡¬í”„íŠ¸ ì œê±°
    if prompt.strip() in result:
        result = result.replace(prompt.strip(), "", 1).strip()

    # 'ìš”ì•½' ì´í›„ ë‚´ìš©ë§Œ ì¶”ì¶œ
    match = re.search(r"(### ìš”ì•½[\s\S]*)", result)
    return match.group(1).strip() if match else result


# âœ… 5. ì „ì²´ ì²˜ë¦¬
def create_training_dataset(input_dir_pattern, output_jsonl):
    file_paths = glob(input_dir_pattern)
    with open(output_jsonl, "w", encoding="utf-8") as f_out:
        for file_path in tqdm(file_paths, desc="ğŸ“‚ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ì§„í–‰"):
            print(f"\nğŸ“ ì²˜ë¦¬ ì¤‘: {file_path}")
            utterances = load_jsonl(file_path)
            chunks = chunk_text(utterances)

            summary_accum = ""
            for idx, (chunk, speakers) in enumerate(tqdm(chunks, desc=f"ğŸ§© ì²­í¬ ì²˜ë¦¬ ({os.path.basename(file_path)})", leave=False)):
                participants_str = ", ".join(speakers)

                prompt = f"""
ë‹¤ìŒì€ íšŒì˜ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.

[ì°¸ì—¬ì]
{participants_str}

[íšŒì˜ ì²­í¬]
{chunk}

[ì´ì „ê¹Œì§€ì˜ ìš”ì•½]
{summary_accum}

1. ì´ íšŒì˜ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜.
2. ì•ˆê±´ì´ ìˆë‹¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•´ì¤˜.
3. ê° ì•ˆê±´ì— ëŒ€í•´ í•„ìš”í•œ ì‘ì—…ë“¤ì„ ë¶„í•´í•´ì¤˜. (ëˆ„ê°€, ë¬´ì—‡ì„, ì–¸ì œê¹Œì§€)

ê²°ê³¼ëŠ” ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¤˜:

### ìš”ì•½
- ...

### ì•ˆê±´
1. ì œëª©: ...
   - ì„¸ë¶€ ì„¤ëª…
   - ê´€ë ¨ ë°œì–¸ì

### ì—…ë¬´ ë¶„í•´
- [ì—…ë¬´]: ë‹´ë‹¹ì, ë§ˆê°ì¼, ê´€ë ¨ ì•ˆê±´
"""
                response = generate(prompt)
                json.dump({
                    "file": os.path.basename(file_path),
                    "chunk_index": idx,
                    "response": response
                }, f_out, ensure_ascii=False)
                f_out.write("\n")
                summary_accum += response + "\n"


# âœ… 6. ì‹¤í–‰ ì§„ì…ì 
if __name__ == "__main__":
    create_training_dataset(
        "label_0_output.jsonl",   # ë˜ëŠ” "data/*.jsonl"
        "250724_v2.jsonl"
    )
