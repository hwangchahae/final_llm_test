# !pip install vllm

import os, json, re
from glob import glob
from tqdm import tqdm
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import torch
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
from functools import partial
import threading

#  1. ëª¨ë¸ ì„ íƒ
model_path = "Qwen/Qwen3-14B-AWQ"
print(f"ğŸš€ ì„ íƒëœ ëª¨ë¸: {model_path}")

# ì „ì—­ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € (í”„ë¡œì„¸ìŠ¤ë³„ë¡œ ì´ˆê¸°í™”)
llm = None
tokenizer = None
sampling_params = None

def initialize_model():
    """ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ëª¨ë¸ ì´ˆê¸°í™”"""
    global llm, tokenizer, sampling_params
    
    if llm is None:
        print(f"ğŸ”§ í”„ë¡œì„¸ìŠ¤ {os.getpid()}ì—ì„œ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # VLLM ì—”ì§„ ì´ˆê¸°í™”
        llm = LLM(
            model=model_path,
            quantization="awq_marlin" if "AWQ" in model_path else None,
            tensor_parallel_size=1,
            max_model_len=16384,
            gpu_memory_utilization=0.7,  # ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì •
            trust_remote_code=True,
            enforce_eager=False,
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
        sampling_params = SamplingParams(
            temperature=0.2,
            max_tokens=2048,
            stop=None,
            skip_special_tokens=True,
        )
        print(f"âœ… í”„ë¡œì„¸ìŠ¤ {os.getpid()} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

def clean_text(text):
    if not text:
        return ""
    
    # íŠ¹ì • íƒœê·¸ë“¤ë§Œ ì œê±°
    text = re.sub(r'\[TGT\]', '', text)
    text = re.sub(r'\[/TGT\]', '', text)

    # ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def load_json_file(file_path):
    """JSON ë˜ëŠ” JSONL íŒŒì¼ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ë¡œë“œ"""
    file_ext = os.path.splitext(file_path)[1].lower()
    
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            if file_ext == '.jsonl':
                data = []
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        json_obj = json.loads(line)
                        if "text" in json_obj:
                            data.append({
                                "speaker": json_obj.get("speaker"),
                                "text": clean_text(json_obj.get("text"))
                            })
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸  JSONL ë¼ì¸ {line_num} íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                return data
                
            elif file_ext == '.json':
                json_data = json.load(f)
                
                if isinstance(json_data, list):
                    data = []
                    for item in json_data:
                        if isinstance(item, dict) and "text" in item:
                            data.append({
                                "speaker": item.get("speaker"),
                                "text": clean_text(item.get("text"))
                            })
                    return data
                    
                elif isinstance(json_data, dict):
                    if "text" in json_data:
                        return [{
                            "speaker": json_data.get("speaker"),
                            "text": clean_text(json_data.get("text"))
                        }]
                    else:
                        data = []
                        for key, value in json_data.items():
                            if isinstance(value, list):
                                for item in value:
                                    if isinstance(item, dict) and "text" in item:
                                        data.append({
                                            "speaker": item.get("speaker"),
                                            "text": clean_text(item.get("text"))
                                        })
                            elif isinstance(value, dict) and "text" in value:
                                data.append({
                                    "speaker": value.get("speaker"),
                                    "text": clean_text(value.get("text"))
                                })
                        return data
                return []
                
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}")
        return []

def chunk_text(utterances, max_tokens=5000, stride=512):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    if not utterances:
        return []
        
    input_ids = []
    metadata = []

    for utt in utterances:
        if not utt["text"]:
            continue
        tokens = tokenizer.encode(utt["text"], add_special_tokens=False)
        input_ids.extend(tokens)
        metadata.extend([utt["speaker"]] * len(tokens))

    if not input_ids:
        return []

    chunks = []
    speakers_per_chunk = []

    i = 0
    while i < len(input_ids):
        chunk_ids = input_ids[i:i + max_tokens]
        chunk_speakers = metadata[i:i + max_tokens]

        chunk_text = tokenizer.decode(chunk_ids)
        unique_speakers = list(set(chunk_speakers))

        chunks.append(chunk_text)
        speakers_per_chunk.append(unique_speakers)

        i += max_tokens - stride

    return list(zip(chunks, speakers_per_chunk))

def get_file_date(file_path):
    """íŒŒì¼ëª… ìš°ì„  â†’ ë©”íƒ€ë°ì´í„° ì°¨ì„ """
    filename = os.path.basename(file_path)
    date_match = re.search(r'(\d{6})', filename)
    if date_match:
        date_str = date_match.group(1)
        try:
            year = "20" + date_str[:2]
            month = date_str[2:4] 
            day = date_str[4:6]
            return f"{year}-{month}-{day}"
        except:
            pass
    
    try:
        mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
        return mtime.strftime("%Y-%m-%d")
    except:
        pass
    
    return datetime.now().strftime("%Y-%m-%d")

def generate_chunk_summary(chunk_data):
    """ê°œë³„ ì²­í¬ ìš”ì•½ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    chunk, speakers, chunk_index, file_date, summary_accum = chunk_data
    
    # ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì´ˆê¸°í™”
    if llm is None:
        initialize_model()
    
    participants_str = ", ".join(speakers) if speakers else "ì•Œ ìˆ˜ ì—†ìŒ"
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    if chunk_index == 0:
        prompt = f"""íšŒì˜ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì°¸ì—¬ì: {participants_str}
íšŒì˜ ë‚ ì§œ: {file_date}

íšŒì˜ ë‚´ìš©:
{chunk}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

### ìš”ì•½
- ì£¼ìš” ë‚´ìš©ì„ 3-5ê°œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½

### ì•ˆê±´
1. ì•ˆê±´ëª…: ì„¤ëª…
2. ì•ˆê±´ëª…: ì„¤ëª…

### ì—…ë¬´ ë¶„í•´
- ì—…ë¬´ë‚´ìš©: ë‹´ë‹¹ì, ë§ˆê°ì¼, ê´€ë ¨ì•ˆê±´

**ì¤‘ìš”**: ë§ˆê°ì¼ì€ {file_date}ë¥¼ ì°¸ê³ í•´ì„œ ê³„ì‚°í•˜ì„¸ìš”."""

    else:
        prompt = f"""ì´ì „ ìš”ì•½ì„ ì°¸ê³ í•˜ì—¬ ì¶”ê°€ íšŒì˜ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì°¸ì—¬ì: {participants_str}
íšŒì˜ ë‚ ì§œ: {file_date}

ì´ì „ ìš”ì•½:
{summary_accum}

ì¶”ê°€ íšŒì˜ ë‚´ìš©:
{chunk}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

### ìš”ì•½
- ì „ì²´ ë‚´ìš© ìš”ì•½ (ì´ì „ + í˜„ì¬)

### ì•ˆê±´
1. ì•ˆê±´ëª…: ì„¤ëª…

### ì—…ë¬´ ë¶„í•´
- ì—…ë¬´ë‚´ìš©: ë‹´ë‹¹ì, ë§ˆê°ì¼, ê´€ë ¨ì•ˆê±´

**ì¤‘ìš”**: ë§ˆê°ì¼ì€ {file_date}ë¥¼ ì°¸ê³ í•´ì„œ ê³„ì‚°í•˜ì„¸ìš”."""
    
    # ìƒì„±
    outputs = llm.generate([prompt], sampling_params)
    result = outputs[0].outputs[0].text.strip()
    
    if chunk_index == 0:
        match = re.search(r"(### ìš”ì•½[\s\S]*)", result)
        return match.group(1).strip() if match else result
    else:
        summary_matches = list(re.finditer(r"### ìš”ì•½", result))
        if len(summary_matches) >= 2:
            return result[summary_matches[1].start():].strip()
        else:
            return result

def process_single_file_parallel(input_file_path, output_dir, model_used, folder_name):
    """ë‹¨ì¼ íŒŒì¼ì„ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ìš”ì•½"""
    
    print(f"\nğŸ“ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘: {input_file_path}")
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ)
    if llm is None:
        initialize_model()
    
    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    output_jsonl = os.path.join(output_dir, f"250730_{model_used}_{folder_name}_summary.jsonl")
    output_txt = os.path.join(output_dir, f"250730_{model_used}_{folder_name}_summary.txt")
    
    file_date = get_file_date(input_file_path)
    
    utterances = load_json_file(input_file_path)
    if not utterances:
        print(f"âš ï¸  {input_file_path}ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
        
    chunks = chunk_text(utterances)
    if not chunks:
        print(f"âš ï¸  {input_file_path}ì—ì„œ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

    # ì²­í¬ë³„ ë³‘ë ¬ ì²˜ë¦¬ (ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•¨ - ì´ì „ ìš”ì•½ í•„ìš”)
    # í•˜ì§€ë§Œ ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ëŠ” ìˆìŒ
    
    with open(output_jsonl, "w", encoding="utf-8") as f_out:
        summary_accum = ""
        
        # ì²­í¬ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (ê° ì²­í¬ëŠ” ì´ì „ ê²°ê³¼ì— ì˜ì¡´)
        for idx, (chunk, speakers) in enumerate(tqdm(chunks, desc=f"ğŸ§© ì²­í¬ ì²˜ë¦¬ ({folder_name})", leave=False)):
            chunk_data = (chunk, speakers, idx, file_date, summary_accum)
            response = generate_chunk_summary(chunk_data)
            
            json.dump({
                "file": os.path.basename(input_file_path),
                "folder": folder_name,
                "chunk_index": idx,
                "file_date": file_date,
                "model": model_used,
                "response": response
            }, f_out, ensure_ascii=False)
            f_out.write("\n")
            summary_accum = response + "\n"
    
    # TXT íŒŒì¼ë¡œ ìµœì¢… ê²°ê³¼ ì €ì¥
    save_final_result_as_txt(output_jsonl, output_txt, folder_name)
    return True

def save_final_result_as_txt(jsonl_file, txt_file, folder_name):
    """ìµœì¢… ê²°ê³¼ë¥¼ TXT íŒŒì¼ë¡œ ì €ì¥"""
    try:
        with open(jsonl_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
        
        if not lines:
            print("âŒ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return
        
        data = json.loads(lines[-1].strip())
        response = data['response']
        file_date = data.get('file_date', 'N/A')
        model_used = data.get('model', 'Unknown')
        
        content = []
        content.append("=" * 80)
        content.append(f"ğŸ“‹ ìµœì¢… íšŒì˜ ìš”ì•½ ê²°ê³¼ - {folder_name}")
        content.append(f"ğŸ“… íšŒì˜ ë‚ ì§œ: {file_date}")
        content.append(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {model_used}")
        content.append(f"ğŸ“‚ ì²˜ë¦¬ í´ë”: {folder_name}")
        content.append("=" * 80)
        content.append("")
        
        sections = response.split('### ')
        for section in sections:
            if not section.strip():
                continue
                
            section = section.strip()
            
            if section.startswith('ìš”ì•½'):
                content.append(f"ğŸ¯ ìš”ì•½")
                content.append("-" * 60)
                text_content = section.split('\n', 1)[1] if '\n' in section else ""
                for line in text_content.split('\n'):
                    if line.strip():
                        content.append(f"  {line.strip()}")
                content.append("")
                        
            elif section.startswith('ì•ˆê±´'):
                content.append(f"ğŸ“Œ ì•ˆê±´")
                content.append("-" * 60)
                text_content = section.split('\n', 1)[1] if '\n' in section else ""
                for line in text_content.split('\n'):
                    if line.strip():
                        if line.strip().startswith(('1.', '2.', '3.', '4.', '5.')):
                            content.append(f"\n  ğŸ“ {line.strip()}")
                        else:
                            content.append(f"     {line.strip()}")
                content.append("")
                            
            elif section.startswith('ì—…ë¬´ ë¶„í•´'):
                content.append(f"âš¡ ì—…ë¬´ ë¶„í•´")
                content.append("-" * 60)
                text_content = section.split('\n', 1)[1] if '\n' in section else ""
                for line in text_content.split('\n'):
                    if line.strip() and line.strip().startswith('-'):
                        content.append(f"  âœ… {line.strip()[1:].strip()}")
                content.append("")
        
        content.append("=" * 80)
        
        with open(txt_file, "w", encoding="utf-8") as f:
            f.write("\n".join(content))
        
        print(f"âœ… ê²°ê³¼ê°€ {txt_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")

def process_file_wrapper(args):
    """ThreadPoolExecutorë¥¼ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    folder_name, folder_path, json_file, model_used = args
    
    try:
        print(f"ğŸ”„ ì²˜ë¦¬ ì‹œì‘: {folder_name} (í”„ë¡œì„¸ìŠ¤ {os.getpid()})")
        result = process_single_file_parallel(json_file, folder_path, model_used, folder_name)
        if result:
            print(f"âœ… {folder_name} ì²˜ë¦¬ ì™„ë£Œ")
            return (folder_name, True, None)
        else:
            print(f"âŒ {folder_name} ì²˜ë¦¬ ì‹¤íŒ¨")
            return (folder_name, False, "ì²˜ë¦¬ ì‹¤íŒ¨")
    except Exception as e:
        print(f"âŒ {folder_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return (folder_name, False, str(e))

def batch_process_folders_parallel(base_dir, model_used, max_workers=None):
    """ë³‘ë ¬ë¡œ ì—¬ëŸ¬ í´ë” ì²˜ë¦¬"""
    
    if not os.path.exists(base_dir):
        print(f"âŒ ê¸°ë³¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_dir}")
        return
    
    # í•˜ìœ„ í´ë”ë“¤ ì°¾ê¸°
    subfolders = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path):
            json_file = os.path.join(item_path, "05_final_result.json")
            if os.path.exists(json_file):
                subfolders.append((item, item_path, json_file, model_used))
            else:
                print(f"âš ï¸  {item} í´ë”ì— 05_final_result.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    if not subfolders:
        print(f"âŒ {base_dir}ì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‚ ì´ {len(subfolders)}ê°œ í´ë”ë¥¼ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤:")
    for folder_name, _, _, _ in subfolders:
        print(f"  - {folder_name}")
    
    # ìµœëŒ€ ì›Œì»¤ ìˆ˜ ê²°ì •
    if max_workers is None:
        # GPU ë©”ëª¨ë¦¬ë¥¼ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ìˆ˜ë¡œ ì œí•œ
        max_workers = min(4, len(subfolders))  # ìµœëŒ€ 4ê°œ í”„ë¡œì„¸ìŠ¤
    
    print(f"ğŸš€ {max_workers}ê°œì˜ ì›Œì»¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")
    
    # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ëª¨ë¸ ì´ˆê¸°í™”
    initialize_model()
    
    success_count = 0
    failed_folders = []
    
    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
    # GPU ë©”ëª¨ë¦¬ ì œì•½ìœ¼ë¡œ ì¸í•´ ProcessPoolExecutor ëŒ€ì‹  ThreadPoolExecutor ì‚¬ìš©
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ì‘ì—… ì œì¶œ
        future_to_folder = {
            executor.submit(process_file_wrapper, args): args[0] 
            for args in subfolders
        }
        
        # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
        with tqdm(total=len(subfolders), desc="ğŸ“ ì „ì²´ í´ë” ì²˜ë¦¬", unit="folder") as pbar:
            for future in as_completed(future_to_folder):
                folder_name = future_to_folder[future]
                try:
                    folder_name, success, error = future.result()
                    if success:
                        success_count += 1
                    else:
                        failed_folders.append((folder_name, error))
                except Exception as e:
                    failed_folders.append((folder_name, str(e)))
                
                pbar.update(1)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ‰ ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"âœ… ì„±ê³µ: {success_count}/{len(subfolders)} í´ë”")
    
    if failed_folders:
        print(f"âŒ ì‹¤íŒ¨í•œ í´ë”ë“¤:")
        for folder, error in failed_folders:
            print(f"  - {folder}: {error}")

# ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    # ëª¨ë¸ëª…ì—ì„œ íŒŒì¼ëª…ìš© ë¬¸ìì—´ ì¶”ì¶œ
    model_used = model_path.split('/')[-1].replace('-', '_').replace('.', '_')
    print(f"ğŸ“ íŒŒì¼ëª…ìš© ëª¨ë¸ëª…: {model_used}")
    
    # ë°°ì¹˜ ì²˜ë¦¬í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬
    base_directory = "/workspace/a_results/a"
    
    print(f"ğŸš€ ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {model_path} ëª¨ë¸ ì‚¬ìš©")
    print(f"ğŸ“‚ ê¸°ë³¸ ë””ë ‰í† ë¦¬: {base_directory}")
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ (ìµœëŒ€ 4ê°œ ì›Œì»¤)
    batch_process_folders_parallel(base_directory, model_used, max_workers=4)