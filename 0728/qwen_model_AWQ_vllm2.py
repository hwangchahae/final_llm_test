# !pip install vllm

import os, json, re
from glob import glob
from tqdm import tqdm
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import torch
from datetime import datetime

# âœ… 1. í•˜ìœ„ ëª¨ë¸ ì˜µì…˜ë“¤ - ì›í•˜ëŠ” ê²ƒìœ¼ë¡œ ì„ íƒí•˜ì„¸ìš”
# ì˜µì…˜ 1: Qwen 7B (ë¹ ë¥´ê³  ê°€ë²¼ì›€) - ì¶”ì²œ!
model_path = "Qwen/Qwen2.5-7B-Instruct-AWQ"

# ì˜µì…˜ 2: Qwen 14B (ì¤‘ê°„ ì„±ëŠ¥)
# model_path = "Qwen/Qwen2.5-14B-Instruct-AWQ" 

# ì˜µì…˜ 3: ë” ì‘ì€ ëª¨ë¸ (ë§¤ìš° ë¹ ë¦„)
# model_path = "Qwen/Qwen2.5-3B-Instruct"


print(f"ğŸš€ ì„ íƒëœ ëª¨ë¸: {model_path}")

# VLLM ì—”ì§„ ì´ˆê¸°í™” (í•˜ìœ„ ëª¨ë¸ìš© ì„¤ì • ìµœì í™”)
llm = LLM(
    model=model_path,
    quantization="awq_marlin" if "AWQ" in model_path else None,  # AWQ ëª¨ë¸ë§Œ quantization ì ìš©
    tensor_parallel_size=1,
    max_model_len=8192,  # í•˜ìœ„ ëª¨ë¸ì€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¤„ì„
    gpu_memory_utilization=0.8,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ê¸ˆ ì¤„ì„
    trust_remote_code=True,
    enforce_eager=False,
)

# í† í¬ë‚˜ì´ì €ëŠ” ë³„ë„ë¡œ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True
)

# í•˜ìœ„ ëª¨ë¸ìš© ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° (ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •)
sampling_params = SamplingParams(
    temperature=0.2,  # ì•½ê°„ ë†’ì—¬ì„œ ì°½ì˜ì„± ì¦ê°€
    max_tokens=1536,  # í† í° ìˆ˜ ì¤„ì„
    stop=None,
    skip_special_tokens=True,
)

def clean_text(text):
    if not text:
        return ""
    
    # íŠ¹ì • íƒœê·¸ë“¤ë§Œ ì œê±°
    text = re.sub(r'\[TGT\]', '', text)
    text = re.sub(r'\[/TGT\]', '', text)

    # ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

# âœ… 2. JSON/JSONL ë¡œë“œ í•¨ìˆ˜ (í™•ì¥ì ìë™ ê°ì§€)
def load_json_file(file_path):
    """JSON ë˜ëŠ” JSONL íŒŒì¼ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ë¡œë“œ"""
    file_ext = os.path.splitext(file_path)[1].lower()
    
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            if file_ext == '.jsonl':
                # JSONL íŒŒì¼ ì²˜ë¦¬
                print(f"ğŸ“„ JSONL íŒŒì¼ë¡œ ì¸ì‹: {os.path.basename(file_path)}")
                data = []
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        json_obj = json.loads(line)
                        if "text" in json_obj:
                            data.append({
                                "speaker": json_obj.get("speaker"),
                                "text": clean_text(json_obj.get("text"))
                            })
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸  JSONL ë¼ì¸ {line_num} íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                return data
                
            elif file_ext == '.json':
                # JSON íŒŒì¼ ì²˜ë¦¬
                print(f"ğŸ“„ JSON íŒŒì¼ë¡œ ì¸ì‹: {os.path.basename(file_path)}")
                json_data = json.load(f)
                
                # JSON êµ¬ì¡° ìë™ ê°ì§€
                if isinstance(json_data, list):
                    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ JSON
                    data = []
                    for item in json_data:
                        if isinstance(item, dict) and "text" in item:
                            data.append({
                                "speaker": item.get("speaker"),
                                "text": clean_text(item.get("text"))
                            })
                    return data
                    
                elif isinstance(json_data, dict):
                    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ JSON
                    if "text" in json_data:
                        # ë‹¨ì¼ ê°ì²´
                        return [{
                            "speaker": json_data.get("speaker"),
                            "text": clean_text(json_data.get("text"))
                        }]
                    else:
                        # í‚¤-ê°’ ìŒì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„° ì°¾ê¸°
                        data = []
                        for key, value in json_data.items():
                            if isinstance(value, list):
                                for item in value:
                                    if isinstance(item, dict) and "text" in item:
                                        data.append({
                                            "speaker": item.get("speaker"),
                                            "text": clean_text(item.get("text"))
                                        })
                            elif isinstance(value, dict) and "text" in value:
                                data.append({
                                    "speaker": value.get("speaker"),
                                    "text": clean_text(value.get("text"))
                                })
                        return data
                else:
                    print(f"âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” JSON êµ¬ì¡°: {type(json_data)}")
                    return []
            else:
                print(f"âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í™•ì¥ì: {file_ext}")
                return []
                
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return []
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return []
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []

# âœ… 3. ì²­í¬ ë¶„í•  (í•˜ìœ„ ëª¨ë¸ìš© - ì²­í¬ í¬ê¸° ì¤„ì„)
def chunk_text(utterances, max_tokens=3000, stride=256):  # ì²­í¬ í¬ê¸° ì¤„ì„
    if not utterances:
        print("âš ï¸  ë¹ˆ ë°ì´í„°ì…ë‹ˆë‹¤.")
        return []
        
    input_ids = []
    metadata = []

    for utt in utterances:
        if not utt["text"]:
            continue
        tokens = tokenizer.encode(utt["text"], add_special_tokens=False)
        input_ids.extend(tokens)
        metadata.extend([utt["speaker"]] * len(tokens))

    if not input_ids:
        print("âš ï¸  í† í°í™”ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []

    chunks = []
    speakers_per_chunk = []

    i = 0
    while i < len(input_ids):
        chunk_ids = input_ids[i:i + max_tokens]
        chunk_speakers = metadata[i:i + max_tokens]

        chunk_text = tokenizer.decode(chunk_ids)
        unique_speakers = list(set(chunk_speakers))

        chunks.append(chunk_text)
        speakers_per_chunk.append(unique_speakers)

        i += max_tokens - stride

    return list(zip(chunks, speakers_per_chunk))

# âœ… 4. íŒŒì¼ ë‚ ì§œ ì¶”ì¶œ í•¨ìˆ˜
def get_file_date(file_path):
    """íŒŒì¼ëª… ìš°ì„  â†’ ë©”íƒ€ë°ì´í„° ì°¨ì„ """
    filename = os.path.basename(file_path)
    date_match = re.search(r'(\d{6})', filename)
    if date_match:
        date_str = date_match.group(1)
        try:
            year = "20" + date_str[:2]
            month = date_str[2:4] 
            day = date_str[4:6]
            return f"{year}-{month}-{day}"
        except:
            pass
    
    try:
        mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
        return mtime.strftime("%Y-%m-%d")
    except:
        pass
    
    return datetime.now().strftime("%Y-%m-%d")

# âœ… 5. ìš”ì•½ ìƒì„± í•¨ìˆ˜ (í•˜ìœ„ ëª¨ë¸ìš© - í”„ë¡¬í”„íŠ¸ ê°„ì†Œí™”)
def generate(prompt, chunk_index):
    outputs = llm.generate([prompt], sampling_params)
    result = outputs[0].outputs[0].text.strip()
    
    if chunk_index == 0:
        match = re.search(r"(### ìš”ì•½[\s\S]*)", result)
        return match.group(1).strip() if match else result
    else:
        summary_matches = list(re.finditer(r"### ìš”ì•½", result))
        if len(summary_matches) >= 2:
            return result[summary_matches[1].start():].strip()
        else:
            return result

# âœ… 6. ì „ì²´ ì²˜ë¦¬ (í•˜ìœ„ ëª¨ë¸ìš© - í”„ë¡¬í”„íŠ¸ ìµœì í™”)
def create_training_dataset(input_dir_pattern, output_jsonl, model_used):
    file_paths = glob(input_dir_pattern)
    
    if not file_paths:
        print(f"âŒ íŒ¨í„´ì— ë§ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_dir_pattern}")
        return
    
    # ì§€ì›ë˜ëŠ” íŒŒì¼ë§Œ í•„í„°ë§
    supported_files = []
    for file_path in file_paths:
        ext = os.path.splitext(file_path)[1].lower()
        if ext in ['.json', '.jsonl']:
            supported_files.append(file_path)
        else:
            print(f"âš ï¸  ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path} (í™•ì¥ì: {ext})")
    
    if not supported_files:
        print("âŒ ì§€ì›ë˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (.json ë˜ëŠ” .jsonl íŒŒì¼ë§Œ ì§€ì›)")
        return
    
    print(f"ğŸ“‚ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ ìˆ˜: {len(supported_files)}")
    
    with open(output_jsonl, "w", encoding="utf-8") as f_out:
        for file_path in tqdm(supported_files, desc="ğŸ“‚ ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ì§„í–‰"):
            print(f"\nğŸ“ ì²˜ë¦¬ ì¤‘: {file_path}")
            
            file_date = get_file_date(file_path)
            print(f"ğŸ“… ê¸°ì¤€ ë‚ ì§œ: {file_date}")
            
            utterances = load_json_file(file_path)
            if not utterances:
                print(f"âš ï¸  {file_path}ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
                
            print(f"ğŸ“Š ë¡œë“œëœ ë°œí™” ìˆ˜: {len(utterances)}")
            
            chunks = chunk_text(utterances)
            if not chunks:
                print(f"âš ï¸  {file_path}ì—ì„œ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue

            summary_accum = ""
            for idx, (chunk, speakers) in enumerate(tqdm(chunks, desc=f"ğŸ§© ì²­í¬ ì²˜ë¦¬ ({os.path.basename(file_path)})", leave=False)):
                participants_str = ", ".join(speakers) if speakers else "ì•Œ ìˆ˜ ì—†ìŒ"

                # í•˜ìœ„ ëª¨ë¸ìš© - ë” ê°„ë‹¨í•˜ê³  ëª…í™•í•œ í”„ë¡¬í”„íŠ¸
                if idx == 0:
                    prompt = f"""íšŒì˜ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì°¸ì—¬ì: {participants_str}
íšŒì˜ ë‚ ì§œ: {file_date}

íšŒì˜ ë‚´ìš©:
{chunk}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

### ìš”ì•½
- ì£¼ìš” ë‚´ìš©ì„ 3-5ê°œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½

### ì•ˆê±´
1. ì•ˆê±´ëª…: ì„¤ëª…
2. ì•ˆê±´ëª…: ì„¤ëª…

### ì—…ë¬´ ë¶„í•´
- ì—…ë¬´ë‚´ìš©: ë‹´ë‹¹ì, ë§ˆê°ì¼(1-2ì£¼ í›„), ê´€ë ¨ì•ˆê±´

**ì¤‘ìš”**: ë§ˆê°ì¼ì€ {file_date} ê¸°ì¤€ 1-2ì£¼ í›„ë¡œ ë‹¤ì–‘í•˜ê²Œ ì„¤ì •í•˜ì„¸ìš”."""

                else:
                    prompt = f"""ì´ì „ ìš”ì•½ì„ ì°¸ê³ í•˜ì—¬ ì¶”ê°€ íšŒì˜ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì°¸ì—¬ì: {participants_str}
íšŒì˜ ë‚ ì§œ: {file_date}

ì´ì „ ìš”ì•½:
{summary_accum}

ì¶”ê°€ íšŒì˜ ë‚´ìš©:
{chunk}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

### ìš”ì•½
- ì „ì²´ ë‚´ìš© ìš”ì•½ (ì´ì „ + í˜„ì¬)

### ì•ˆê±´
1. ì•ˆê±´ëª…: ì„¤ëª…

### ì—…ë¬´ ë¶„í•´
- ì—…ë¬´ë‚´ìš©: ë‹´ë‹¹ì, ë§ˆê°ì¼(1-2ì£¼ í›„), ê´€ë ¨ì•ˆê±´

**ì¤‘ìš”**: ë§ˆê°ì¼ì€ {file_date} ê¸°ì¤€ 1-2ì£¼ í›„ë¡œ ë‹¤ì–‘í•˜ê²Œ ì„¤ì •í•˜ì„¸ìš”."""
                
                response = generate(prompt, idx)
                json.dump({
                    "file": os.path.basename(file_path),
                    "chunk_index": idx,
                    "file_date": file_date,
                    "model": model_used,  # íŒŒë¼ë¯¸í„°ë¡œ ë°›ì€ model_used ì‚¬ìš©
                    "response": response
                }, f_out, ensure_ascii=False)
                f_out.write("\n")
                summary_accum = response + "\n"

def save_final_result_as_txt(output_file, txt_file):
    try:
        with open(output_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
        
        if not lines:
            print("âŒ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return
        
        data = json.loads(lines[-1].strip())
        response = data['response']
        file_date = data.get('file_date', 'N/A')
        model_used = data.get('model', 'Unknown')
        
        content = []
        content.append("=" * 80)
        content.append(f"ğŸ“‹ ìµœì¢… íšŒì˜ ìš”ì•½ ê²°ê³¼ (ì²­í¬ {data['chunk_index']})")
        content.append(f"ğŸ“… íšŒì˜ ë‚ ì§œ: {file_date}")
        content.append(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {model_used}")
        content.append("=" * 80)
        content.append("")
        
        sections = response.split('### ')
        for section in sections:
            if not section.strip():
                continue
                
            section = section.strip()
            
            if section.startswith('ìš”ì•½'):
                content.append(f"ğŸ¯ ìš”ì•½")
                content.append("-" * 60)
                text_content = section.split('\n', 1)[1] if '\n' in section else ""
                for line in text_content.split('\n'):
                    if line.strip():
                        content.append(f"  {line.strip()}")
                content.append("")
                        
            elif section.startswith('ì•ˆê±´'):
                content.append(f"ğŸ“Œ ì•ˆê±´")
                content.append("-" * 60)
                text_content = section.split('\n', 1)[1] if '\n' in section else ""
                for line in text_content.split('\n'):
                    if line.strip():
                        if line.strip().startswith(('1.', '2.', '3.', '4.', '5.')):
                            content.append(f"\n  ğŸ“ {line.strip()}")
                        else:
                            content.append(f"     {line.strip()}")
                content.append("")
                            
            elif section.startswith('ì—…ë¬´ ë¶„í•´'):
                content.append(f"âš¡ ì—…ë¬´ ë¶„í•´")
                content.append("-" * 60)
                text_content = section.split('\n', 1)[1] if '\n' in section else ""
                for line in text_content.split('\n'):
                    if line.strip() and line.strip().startswith('-'):
                        content.append(f"  âœ… {line.strip()[1:].strip()}")
                content.append("")
        
        content.append("=" * 80)
        
        with open(txt_file, "w", encoding="utf-8") as f:
            f.write("\n".join(content))
        
        print(f"âœ… ê²°ê³¼ê°€ {txt_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ¤– ì‚¬ìš©ëœ ëª¨ë¸: {model_used}")
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")

# âœ… 8. ì‹¤í–‰
if __name__ == "__main__":
    # ëª¨ë¸ëª…ì—ì„œ íŒŒì¼ëª…ìš© ë¬¸ìì—´ ì¶”ì¶œ
    model_used = model_path.split('/')[-1].replace('-', '_').replace('.', '_')
    print(f"ğŸ“ íŒŒì¼ëª…ìš© ëª¨ë¸ëª…: {model_used}")
    
    # ì…ë ¥ íŒŒì¼ íŒ¨í„´ì„ JSON/JSONL ëª¨ë‘ ì§€ì›í•˜ë„ë¡ ìˆ˜ì •
    input_pattern = "/workspace/05_final_result.json" 
    output_file = f"/workspace/250729_{model_used}_data1_output1.jsonl"
    txt_file = f"/workspace/250729_{model_used}_data1_output_final1.txt"
    
    print(f"ğŸš€ ì‹œì‘: {model_path} ëª¨ë¸ ì‚¬ìš©")
    print(f"ğŸ“‚ ì…ë ¥ íŒ¨í„´: {input_pattern}")
    print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
    print(f"ğŸ“„ ìµœì¢… íŒŒì¼: {txt_file}")
    
    create_training_dataset(input_pattern, output_file, model_used)
    save_final_result_as_txt(output_file, txt_file)