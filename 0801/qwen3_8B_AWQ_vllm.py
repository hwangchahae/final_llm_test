# !pip install vllm

import os, json, re
from glob import glob
from tqdm import tqdm
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
import torch
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
from functools import partial
import threading

#  1. ëª¨ë¸ ì„ íƒ
model_path = "Qwen/Qwen3-8B-AWQ"
print(f"ğŸš€ ì„ íƒëœ ëª¨ë¸: {model_path}")

# ì „ì—­ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € (í”„ë¡œì„¸ìŠ¤ë³„ë¡œ ì´ˆê¸°í™”)
llm = None
tokenizer = None
sampling_params = None

def initialize_model():
    """ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ëª¨ë¸ ì´ˆê¸°í™”"""
    global llm, tokenizer, sampling_params
    
    if llm is None:
        print(f"ğŸ”§ í”„ë¡œì„¸ìŠ¤ {os.getpid()}ì—ì„œ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # VLLM ì—”ì§„ ì´ˆê¸°í™”
        llm = LLM(
            model=model_path,
            quantization="awq_marlin" if "AWQ" in model_path else None,
            tensor_parallel_size=1,
            max_model_len=16384,
            gpu_memory_utilization=0.7,  # ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì •
            trust_remote_code=True,
            enforce_eager=False,
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
        sampling_params = SamplingParams(
            temperature=0.2,
            max_tokens=2048,
            stop=None,
            skip_special_tokens=True,
        )
        print(f"âœ… í”„ë¡œì„¸ìŠ¤ {os.getpid()} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

def generate_notion_project_prompt(meeting_transcript: str) -> str:
    """ë…¸ì…˜ ê¸°íšì•ˆ ìƒì„± í”„ë¡¬í”„íŠ¸"""
    return f"""ë‹¤ìŒ íšŒì˜ ì „ì‚¬ë³¸ì„ ë°”íƒ•ìœ¼ë¡œ ë…¸ì…˜ì— ì—…ë¡œë“œí•  í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ì‘ì„±í•˜ì„¸ìš”.

**íšŒì˜ ì „ì‚¬ë³¸:**
{meeting_transcript}

**ì‘ì„± ì§€ì¹¨:**
1. íšŒì˜ì—ì„œ ë…¼ì˜ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ì¸ ê¸°íšì•ˆì„ ì‘ì„±
2. í”„ë¡œì íŠ¸ëª…ì€ íšŒì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆíˆ ëª…ëª…
3. ëª©ì ê³¼ ëª©í‘œëŠ” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±
4. ì‹¤í–‰ ê³„íšì€ ì‹¤í˜„ ê°€ëŠ¥í•œ ë‹¨ê³„ë³„ë¡œ êµ¬ì„±
5. ê¸°ëŒ€ íš¨ê³¼ëŠ” ì •ëŸ‰ì /ì •ì„±ì  ê²°ê³¼ë¥¼ í¬í•¨
6. ëª¨ë“  ë‚´ìš©ì€ í•œêµ­ì–´ë¡œ ì‘ì„±

**ì‘ë‹µ í˜•ì‹:**
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "project_name": "í”„ë¡œì íŠ¸ëª…",
    "project_purpose": "í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©ì ",
    "project_period": "ì˜ˆìƒ ìˆ˜í–‰ ê¸°ê°„ (ì˜ˆ: 2025.01.01 ~ 2025.03.31)",
    "project_manager": "ë‹´ë‹¹ìëª… (íšŒì˜ì—ì„œ ì–¸ê¸‰ëœ ê²½ìš°)",
    "core_objectives": [
        "ëª©í‘œ 1: êµ¬ì²´ì ì¸ ëª©í‘œ",
        "ëª©í‘œ 2: êµ¬ì²´ì ì¸ ëª©í‘œ",
        "ëª©í‘œ 3: êµ¬ì²´ì ì¸ ëª©í‘œ"
    ],
    "core_idea": "í•µì‹¬ ì•„ì´ë””ì–´ ì„¤ëª…",
    "idea_description": "ì•„ì´ë””ì–´ì˜ ê¸°ìˆ ì /ë¹„ì¦ˆë‹ˆìŠ¤ì  ì„¤ëª…",
    "execution_plan": "ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšê³¼ ì¼ì •",
    "expected_effects": [
        "ê¸°ëŒ€íš¨ê³¼ 1: ìì„¸í•œ ì„¤ëª…",
        "ê¸°ëŒ€íš¨ê³¼ 2: ìì„¸í•œ ì„¤ëª…",
        "ê¸°ëŒ€íš¨ê³¼ 3: ìì„¸í•œ ì„¤ëª…"
    ]
}}"""

def clean_text(text):
    if not text:
        return ""
    
    # íŠ¹ì • íƒœê·¸ë“¤ë§Œ ì œê±°
    text = re.sub(r'\[TGT\]', '', text)
    text = re.sub(r'\[/TGT\]', '', text)

    # ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def load_json_file(file_path):
    """JSON ë˜ëŠ” JSONL íŒŒì¼ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ë¡œë“œ"""
    file_ext = os.path.splitext(file_path)[1].lower()
    
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            if file_ext == '.jsonl':
                data = []
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        json_obj = json.loads(line)
                        if "text" in json_obj:
                            data.append({
                                "speaker": json_obj.get("speaker"),
                                "text": clean_text(json_obj.get("text"))
                            })
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸  JSONL ë¼ì¸ {line_num} íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue
                return data
                
            elif file_ext == '.json':
                json_data = json.load(f)
                
                if isinstance(json_data, list):
                    data = []
                    for item in json_data:
                        if isinstance(item, dict) and "text" in item:
                            data.append({
                                "speaker": item.get("speaker"),
                                "text": clean_text(item.get("text"))
                            })
                    return data
                    
                elif isinstance(json_data, dict):
                    if "text" in json_data:
                        return [{
                            "speaker": json_data.get("speaker"),
                            "text": clean_text(json_data.get("text"))
                        }]
                    else:
                        data = []
                        for key, value in json_data.items():
                            if isinstance(value, list):
                                for item in value:
                                    if isinstance(item, dict) and "text" in item:
                                        data.append({
                                            "speaker": item.get("speaker"),
                                            "text": clean_text(item.get("text"))
                                        })
                            elif isinstance(value, dict) and "text" in value:
                                data.append({
                                    "speaker": value.get("speaker"),
                                    "text": clean_text(value.get("text"))
                                })
                        return data
                return []
                
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}")
        return []

def chunk_text(utterances, max_tokens=5000, stride=512):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    if not utterances:
        return []
        
    input_ids = []
    metadata = []

    for utt in utterances:
        if not utt["text"]:
            continue
        tokens = tokenizer.encode(utt["text"], add_special_tokens=False)
        input_ids.extend(tokens)
        metadata.extend([utt["speaker"]] * len(tokens))

    if not input_ids:
        return []

    chunks = []
    speakers_per_chunk = []

    i = 0
    while i < len(input_ids):
        chunk_ids = input_ids[i:i + max_tokens]
        chunk_speakers = metadata[i:i + max_tokens]

        chunk_text = tokenizer.decode(chunk_ids)
        unique_speakers = list(set(chunk_speakers))

        chunks.append(chunk_text)
        speakers_per_chunk.append(unique_speakers)

        i += max_tokens - stride

    return list(zip(chunks, speakers_per_chunk))

def get_file_date(file_path):
    """íŒŒì¼ëª… ìš°ì„  â†’ ë©”íƒ€ë°ì´í„° ì°¨ì„ """
    filename = os.path.basename(file_path)
    date_match = re.search(r'(\d{6})', filename)
    if date_match:
        date_str = date_match.group(1)
        try:
            year = "20" + date_str[:2]
            month = date_str[2:4] 
            day = date_str[4:6]
            return f"{year}-{month}-{day}"
        except:
            pass
    
    try:
        mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
        return mtime.strftime("%Y-%m-%d")
    except:
        pass
    
    return datetime.now().strftime("%Y-%m-%d")

def generate_notion_project_plan(chunk_data):
    """ê°œë³„ ì²­í¬ì—ì„œ ë…¸ì…˜ í”„ë¡œì íŠ¸ ê¸°íšì•ˆ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    chunk, speakers, chunk_index, file_date, project_accum = chunk_data
    
    # ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì´ˆê¸°í™”
    if llm is None:
        initialize_model()
    
    participants_str = ", ".join(speakers) if speakers else "ì•Œ ìˆ˜ ì—†ìŒ"
    
    # ì „ì²´ íšŒì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ í”„ë¡œì íŠ¸ ê¸°íšì•ˆìœ¼ë¡œ ìƒì„±
    if chunk_index == 0:
        # ì²« ë²ˆì§¸ ì²­í¬: ì´ˆê¸° í”„ë¡œì íŠ¸ ê¸°íšì•ˆ ìƒì„±
        meeting_transcript = f"ì°¸ì—¬ì: {participants_str}\níšŒì˜ ë‚ ì§œ: {file_date}\n\níšŒì˜ ë‚´ìš©:\n{chunk}"
        prompt = generate_notion_project_prompt(meeting_transcript)
    else:
        # ì¶”ê°€ ì²­í¬: ê¸°ì¡´ ê¸°íšì•ˆì„ ì—…ë°ì´íŠ¸
        meeting_transcript = f"ì°¸ì—¬ì: {participants_str}\níšŒì˜ ë‚ ì§œ: {file_date}\n\nì¶”ê°€ íšŒì˜ ë‚´ìš©:\n{chunk}"
        prompt = f"""ì´ì „ì— ìƒì„±ëœ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ ë‹¤ìŒ ì¶”ê°€ íšŒì˜ ë‚´ìš©ì„ ë°˜ì˜í•˜ì—¬ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”.

**ê¸°ì¡´ í”„ë¡œì íŠ¸ ê¸°íšì•ˆ:**
{project_accum}

**ì¶”ê°€ íšŒì˜ ë‚´ìš©:**
{meeting_transcript}

**ì—…ë°ì´íŠ¸ ì§€ì¹¨:**
1. ê¸°ì¡´ ê¸°íšì•ˆì˜ ë‚´ìš©ì„ ìœ ì§€í•˜ë©´ì„œ ìƒˆë¡œìš´ ì •ë³´ë¥¼ í†µí•©
2. ì¤‘ë³µë˜ëŠ” ë‚´ìš©ì€ ë³‘í•©í•˜ê³  ìƒì¶©í•˜ëŠ” ë‚´ìš©ì€ ìµœì‹  ì •ë³´ë¥¼ ìš°ì„ 
3. í”„ë¡œì íŠ¸ì˜ ì „ì²´ì ì¸ ì¼ê´€ì„±ì„ ìœ ì§€

**ì‘ë‹µ í˜•ì‹:**
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸ëœ ì „ì²´ ê¸°íšì•ˆì„ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "project_name": "í”„ë¡œì íŠ¸ëª…",
    "project_purpose": "í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©ì ",
    "project_period": "ì˜ˆìƒ ìˆ˜í–‰ ê¸°ê°„ (ì˜ˆ: 2025.01.01 ~ 2025.03.31)",
    "project_manager": "ë‹´ë‹¹ìëª… (íšŒì˜ì—ì„œ ì–¸ê¸‰ëœ ê²½ìš°)",
    "core_objectives": [
        "ëª©í‘œ 1: êµ¬ì²´ì ì¸ ëª©í‘œ",
        "ëª©í‘œ 2: êµ¬ì²´ì ì¸ ëª©í‘œ",
        "ëª©í‘œ 3: êµ¬ì²´ì ì¸ ëª©í‘œ"
    ],
    "core_idea": "í•µì‹¬ ì•„ì´ë””ì–´ ì„¤ëª…",
    "idea_description": "ì•„ì´ë””ì–´ì˜ ê¸°ìˆ ì /ë¹„ì¦ˆë‹ˆìŠ¤ì  ì„¤ëª…",
    "execution_plan": "ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšê³¼ ì¼ì •",
    "expected_effects": [
        "ê¸°ëŒ€íš¨ê³¼ 1: ìì„¸í•œ ì„¤ëª…",
        "ê¸°ëŒ€íš¨ê³¼ 2: ìì„¸í•œ ì„¤ëª…",
        "ê¸°ëŒ€íš¨ê³¼ 3: ìì„¸í•œ ì„¤ëª…"
    ]
}}"""
    
    # ìƒì„±
    outputs = llm.generate([prompt], sampling_params)
    result = outputs[0].outputs[0].text.strip()
    
    return result

def process_single_file_parallel(input_file_path, output_dir, model_used, folder_name):
    """ë‹¨ì¼ íŒŒì¼ì„ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë…¸ì…˜ í”„ë¡œì íŠ¸ ê¸°íšì•ˆ ìƒì„±"""
    
    print(f"\nğŸ“ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘: {input_file_path}")
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ)
    if llm is None:
        initialize_model()
    
    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    output_jsonl = os.path.join(output_dir, f"250730_{model_used}_{folder_name}_notion_project.jsonl")
    output_txt = os.path.join(output_dir, f"250730_{model_used}_{folder_name}_notion_project.txt")
    
    file_date = get_file_date(input_file_path)
    
    utterances = load_json_file(input_file_path)
    if not utterances:
        print(f"âš ï¸  {input_file_path}ì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
        
    chunks = chunk_text(utterances)
    if not chunks:
        print(f"âš ï¸  {input_file_path}ì—ì„œ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False

    # ì²­í¬ë³„ ë³‘ë ¬ ì²˜ë¦¬ (ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•¨ - ì´ì „ ê²°ê³¼ í•„ìš”)
    with open(output_jsonl, "w", encoding="utf-8") as f_out:
        project_accum = ""
        
        # ì²­í¬ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (ê° ì²­í¬ëŠ” ì´ì „ ê²°ê³¼ì— ì˜ì¡´)
        for idx, (chunk, speakers) in enumerate(tqdm(chunks, desc=f"ğŸ§© ì²­í¬ ì²˜ë¦¬ ({folder_name})", leave=False)):
            chunk_data = (chunk, speakers, idx, file_date, project_accum)
            response = generate_notion_project_plan(chunk_data)
            
            json.dump({
                "file": os.path.basename(input_file_path),
                "folder": folder_name,
                "chunk_index": idx,
                "file_date": file_date,
                "model": model_used,
                "response": response
            }, f_out, ensure_ascii=False)
            f_out.write("\n")
            project_accum = response + "\n"
    
    # TXT íŒŒì¼ë¡œ ìµœì¢… ê²°ê³¼ ì €ì¥
    save_final_result_as_txt(output_jsonl, output_txt, folder_name)
    return True

def save_final_result_as_txt(jsonl_file, txt_file, folder_name):
    """ìµœì¢… ë…¸ì…˜ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì„ TXT íŒŒì¼ë¡œ ì €ì¥"""
    try:
        with open(jsonl_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
        
        if not lines:
            print("âŒ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return
        
        data = json.loads(lines[-1].strip())
        response = data['response']
        file_date = data.get('file_date', 'N/A')
        model_used = data.get('model', 'Unknown')
        
        content = []
        content.append("=" * 80)
        content.append(f"ğŸ“‹ ë…¸ì…˜ í”„ë¡œì íŠ¸ ê¸°íšì•ˆ - {folder_name}")
        content.append(f"ğŸ“… íšŒì˜ ë‚ ì§œ: {file_date}")
        content.append(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {model_used}")
        content.append(f"ğŸ“‚ ì²˜ë¦¬ í´ë”: {folder_name}")
        content.append("=" * 80)
        content.append("")
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            # JSON í˜•íƒœë¡œ íŒŒì‹±
            project_data = json.loads(response)
            
            content.append(f"ğŸ¯ í”„ë¡œì íŠ¸ëª…: {project_data.get('project_name', 'N/A')}")
            content.append("-" * 60)
            content.append("")
            
            content.append(f"ğŸ“Œ í”„ë¡œì íŠ¸ ëª©ì ")
            content.append(f"  {project_data.get('project_purpose', 'N/A')}")
            content.append("")
            
            content.append(f"ğŸ“… í”„ë¡œì íŠ¸ ê¸°ê°„")
            content.append(f"  {project_data.get('project_period', 'N/A')}")
            content.append("")
            
            content.append(f"ğŸ‘¤ í”„ë¡œì íŠ¸ ë‹´ë‹¹ì")
            content.append(f"  {project_data.get('project_manager', 'N/A')}")
            content.append("")
            
            content.append(f"ğŸ¯ í•µì‹¬ ëª©í‘œ")
            objectives = project_data.get('core_objectives', [])
            for i, obj in enumerate(objectives, 1):
                content.append(f"  {i}. {obj}")
            content.append("")
            
            content.append(f"ğŸ’¡ í•µì‹¬ ì•„ì´ë””ì–´")
            content.append(f"  {project_data.get('core_idea', 'N/A')}")
            content.append("")
            
            content.append(f"ğŸ“– ì•„ì´ë””ì–´ ìƒì„¸ ì„¤ëª…")
            content.append(f"  {project_data.get('idea_description', 'N/A')}")
            content.append("")
            
            content.append(f"âš¡ ì‹¤í–‰ ê³„íš")
            content.append(f"  {project_data.get('execution_plan', 'N/A')}")
            content.append("")
            
            content.append(f"ğŸŠ ê¸°ëŒ€ íš¨ê³¼")
            effects = project_data.get('expected_effects', [])
            for i, effect in enumerate(effects, 1):
                content.append(f"  {i}. {effect}")
            content.append("")
            
        except json.JSONDecodeError:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¶œë ¥
            content.append("ğŸ“ í”„ë¡œì íŠ¸ ê¸°íšì•ˆ ë‚´ìš©:")
            content.append("-" * 60)
            content.append(response)
            content.append("")
        
        content.append("=" * 80)
        
        with open(txt_file, "w", encoding="utf-8") as f:
            f.write("\n".join(content))
        
        print(f"âœ… ë…¸ì…˜ í”„ë¡œì íŠ¸ ê¸°íšì•ˆì´ {txt_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")

def process_file_wrapper(args):
    """ThreadPoolExecutorë¥¼ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    folder_name, folder_path, json_file, model_used = args
    
    try:
        print(f"ğŸ”„ ì²˜ë¦¬ ì‹œì‘: {folder_name} (í”„ë¡œì„¸ìŠ¤ {os.getpid()})")
        result = process_single_file_parallel(json_file, folder_path, model_used, folder_name)
        if result:
            print(f"âœ… {folder_name} ì²˜ë¦¬ ì™„ë£Œ")
            return (folder_name, True, None)
        else:
            print(f"âŒ {folder_name} ì²˜ë¦¬ ì‹¤íŒ¨")
            return (folder_name, False, "ì²˜ë¦¬ ì‹¤íŒ¨")
    except Exception as e:
        print(f"âŒ {folder_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return (folder_name, False, str(e))

def batch_process_folders_parallel(base_dir, model_used, max_workers=None):
    """ë³‘ë ¬ë¡œ ì—¬ëŸ¬ í´ë” ì²˜ë¦¬"""
    
    if not os.path.exists(base_dir):
        print(f"âŒ ê¸°ë³¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_dir}")
        return
    
    # í•˜ìœ„ í´ë”ë“¤ ì°¾ê¸°
    subfolders = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path):
            json_file = os.path.join(item_path, "05_final_result.json")
            if os.path.exists(json_file):
                subfolders.append((item, item_path, json_file, model_used))
            else:
                print(f"âš ï¸  {item} í´ë”ì— 05_final_result.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    if not subfolders:
        print(f"âŒ {base_dir}ì—ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“‚ ì´ {len(subfolders)}ê°œ í´ë”ë¥¼ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤:")
    for folder_name, _, _, _ in subfolders:
        print(f"  - {folder_name}")
    
    # ìµœëŒ€ ì›Œì»¤ ìˆ˜ ê²°ì •
    if max_workers is None:
        # GPU ë©”ëª¨ë¦¬ë¥¼ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ìˆ˜ë¡œ ì œí•œ
        max_workers = min(4, len(subfolders))  # ìµœëŒ€ 4ê°œ í”„ë¡œì„¸ìŠ¤
    
    print(f"ğŸš€ {max_workers}ê°œì˜ ì›Œì»¤ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")
    
    # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ëª¨ë¸ ì´ˆê¸°í™”
    initialize_model()
    
    success_count = 0
    failed_folders = []
    
    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
    # GPU ë©”ëª¨ë¦¬ ì œì•½ìœ¼ë¡œ ì¸í•´ ProcessPoolExecutor ëŒ€ì‹  ThreadPoolExecutor ì‚¬ìš©
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  ì‘ì—… ì œì¶œ
        future_to_folder = {
            executor.submit(process_file_wrapper, args): args[0] 
            for args in subfolders
        }
        
        # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
        with tqdm(total=len(subfolders), desc="ğŸ“ ì „ì²´ í´ë” ì²˜ë¦¬", unit="folder") as pbar:
            for future in as_completed(future_to_folder):
                folder_name = future_to_folder[future]
                try:
                    folder_name, success, error = future.result()
                    if success:
                        success_count += 1
                    else:
                        failed_folders.append((folder_name, error))
                except Exception as e:
                    failed_folders.append((folder_name, str(e)))
                
                pbar.update(1)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ‰ ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"âœ… ì„±ê³µ: {success_count}/{len(subfolders)} í´ë”")
    
    if failed_folders:
        print(f"âŒ ì‹¤íŒ¨í•œ í´ë”ë“¤:")
        for folder, error in failed_folders:
            print(f"  - {folder}: {error}")

# ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    # ëª¨ë¸ëª…ì—ì„œ íŒŒì¼ëª…ìš© ë¬¸ìì—´ ì¶”ì¶œ
    model_used = model_path.split('/')[-1].replace('-', '_').replace('.', '_')
    print(f"ğŸ“ íŒŒì¼ëª…ìš© ëª¨ë¸ëª…: {model_used}")
    
    # ë°°ì¹˜ ì²˜ë¦¬í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬
    base_directory = "/workspace/a_results/a"
    
    print(f"ğŸš€ ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {model_path} ëª¨ë¸ ì‚¬ìš©")
    print(f"ğŸ“‚ ê¸°ë³¸ ë””ë ‰í† ë¦¬: {base_directory}")
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ (ìµœëŒ€ 4ê°œ ì›Œì»¤)
    batch_process_folders_parallel(base_directory, model_used, max_workers=4)